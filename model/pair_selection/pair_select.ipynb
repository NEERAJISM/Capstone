{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc87b17-1206-4ccc-a7c4-bb007d217974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import sys \n",
    "from typing import List, Optional, Callable, Dict, Union\n",
    "from collections import defaultdict\n",
    "import calendar\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# This code was developed with the assistance of multiple AI agents (e.g., Perplexity, August 2025).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310d41cc-6991-45c0-a880-2d038550b758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\mangl\\\\Desktop\\\\capstone\\\\pair_selection'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d6597c-bc12-4caf-b831-2bc7d87a8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_map = {\n",
    "  \"<ticker>\": \"ticker\",\n",
    "  \"<date>\": \"date\",\n",
    "  \"<time>\": \"time\",\n",
    "  \"<open>\": \"open_price\",\n",
    "  \"<high>\": \"high_price\",\n",
    "  \"<low>\": \"low_price\",\n",
    "  \"<close>\": \"close_price\",\n",
    "  \"<volume>\": \"volume\",\n",
    "  \"<o/i>\": \"open_interest\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91952f72-629f-48dc-9d0b-e8ece5917208",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(f\"../downloaded_files/Cash Data January 2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7228a9-4779-403a-a105-9adc18718794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;ticker&gt;</th>\n",
       "      <th>&lt;date&gt;</th>\n",
       "      <th>&lt;time&gt;</th>\n",
       "      <th>&lt;open&gt;</th>\n",
       "      <th>&lt;high&gt;</th>\n",
       "      <th>&lt;low&gt;</th>\n",
       "      <th>&lt;close&gt;</th>\n",
       "      <th>&lt;volume&gt;</th>\n",
       "      <th>&lt;o/i&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>09:45:00</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>09:57:00</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>09:59:00</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>10:10:00</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>10:13:00</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/29/2021</td>\n",
       "      <td>13:27:00</td>\n",
       "      <td>11.10</td>\n",
       "      <td>11.10</td>\n",
       "      <td>11.10</td>\n",
       "      <td>11.10</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/29/2021</td>\n",
       "      <td>14:47:00</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/29/2021</td>\n",
       "      <td>15:26:00</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/29/2021</td>\n",
       "      <td>15:28:00</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/29/2021</td>\n",
       "      <td>15:49:00</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>526 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       <ticker>      <date>    <time>  <open>  <high>  <low>  <close>  \\\n",
       "0    21STCENMGM  01/01/2021  09:45:00   10.65   10.65  10.65    10.65   \n",
       "1    21STCENMGM  01/01/2021  09:57:00   10.65   10.65  10.65    10.65   \n",
       "2    21STCENMGM  01/01/2021  09:59:00   10.65   10.65  10.65    10.65   \n",
       "3    21STCENMGM  01/01/2021  10:10:00   10.65   10.65  10.65    10.65   \n",
       "4    21STCENMGM  01/01/2021  10:13:00   10.65   10.65  10.65    10.65   \n",
       "..          ...         ...       ...     ...     ...    ...      ...   \n",
       "521  21STCENMGM  01/29/2021  13:27:00   11.10   11.10  11.10    11.10   \n",
       "522  21STCENMGM  01/29/2021  14:47:00   11.15   11.15  11.15    11.15   \n",
       "523  21STCENMGM  01/29/2021  15:26:00   11.15   11.15  11.15    11.15   \n",
       "524  21STCENMGM  01/29/2021  15:28:00   11.15   11.15  11.15    11.15   \n",
       "525  21STCENMGM  01/29/2021  15:49:00   11.15   11.15  11.15    11.15   \n",
       "\n",
       "     <volume>  <o/i>   \n",
       "0          11       0  \n",
       "1          35       0  \n",
       "2          90       0  \n",
       "3          10       0  \n",
       "4         139       0  \n",
       "..        ...     ...  \n",
       "521       299       0  \n",
       "522         3       0  \n",
       "523        20       0  \n",
       "524         8       0  \n",
       "525         1       0  \n",
       "\n",
       "[526 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(DATA_DIR/'21STCENMGM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a6f1794-2b25-4f38-a549-ec9ece9000f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.CNX100',\n",
       " '.CNXIT',\n",
       " '.NSEBANK',\n",
       " '.NSEI',\n",
       " '20MICRONS',\n",
       " '21STCENMGM',\n",
       " '3IINFOTECH',\n",
       " '3MINDIA',\n",
       " '3PLAND',\n",
       " '5PAISA']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = [i.rsplit(\".\", maxsplit=1)[0] for i in os.listdir(DATA_DIR)]\n",
    "tickers[:10] # few available tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6feee945-6512-4691-9b4f-17ef5c72a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stock_data(\n",
    "    tickers,\n",
    "    start=None,\n",
    "    end=None,\n",
    "    agg_func=None,\n",
    "    resample_freq=None,\n",
    "    columns=None,\n",
    "    impute=True,\n",
    "    DATA_DIR=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Load and process intraday stock data from CSV files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tickers : List[str]\n",
    "        List of stock ticker symbols (without file extensions).\n",
    "    start : str or pd.Timestamp, optional\n",
    "        Start datetime for filtering the data.\n",
    "    end : str or pd.Timestamp, optional\n",
    "        End datetime for filtering the data.\n",
    "    agg_func : str or callable, optional\n",
    "        Aggregation function to apply during resampling (e.g., 'mean', 'ohlc').\n",
    "        Required if `resample_freq` is provided.\n",
    "    resample_freq : str, optional\n",
    "        Resample frequency (e.g., '5min', '15min').\n",
    "    columns : List[str], optional\n",
    "        List of columns to retain from the original data.\n",
    "    impute : bool, default True\n",
    "        Whether to impute missing data using expanding median.\n",
    "    DATA_DIR : str or Path, optional\n",
    "        Directory containing the CSV files.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, pd.DataFrame]\n",
    "        Dictionary mapping each ticker to its corresponding processed DataFrame.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    start = pd.to_datetime(start) if start is not None else None\n",
    "    end = pd.to_datetime(end) if end is not None else None\n",
    "    \n",
    "    # Handle columns parameter - if None, use all available columns\n",
    "    if columns is None:\n",
    "        columns = list(cols_map.keys())\n",
    "    \n",
    "    # Map requested columns to CSV column names\n",
    "    csv_cols = [cols_map.get(col, col) for col in columns if col in cols_map]\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        # Read CSV with specified columns\n",
    "        df = pd.read_csv(Path(DATA_DIR) / f\"{ticker}.csv\", usecols=csv_cols)\n",
    "        \n",
    "        # Rename columns from CSV format to standard format\n",
    "        rename_dict = {v: k for k, v in cols_map.items() if k in columns and v in df.columns}\n",
    "        df = df.rename(columns=rename_dict)\n",
    "        \n",
    "        # Create datetime column\n",
    "        df[\"datetime\"] = pd.to_datetime(df[\"date\"] + \" \" + df[\"time\"], format=\"%m/%d/%Y %H:%M:%S\")\n",
    "        df = df.sort_values(by=\"datetime\")\n",
    "        \n",
    "        # Clean up columns\n",
    "        df = df.drop(columns=[\"ticker\", \"date\", \"time\"], errors=\"ignore\")\n",
    "        df = df.set_index(\"datetime\")\n",
    "        df = df.sort_index()\n",
    "            \n",
    "        # Force imputation if resampling\n",
    "        if resample_freq:\n",
    "            impute = True\n",
    "            \n",
    "        # Impute missing data\n",
    "        if impute:    \n",
    "            full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq=\"1min\")\n",
    "            df = df.reindex(full_index)\n",
    "            for col in df.columns:\n",
    "                df[col] = df[col].fillna(df[col].expanding().median())\n",
    "        \n",
    "        # Filter by date range\n",
    "        if start is not None:\n",
    "            df = df[df.index >= start]\n",
    "        if end is not None:\n",
    "            df = df[df.index <= end]\n",
    "        \n",
    "        # Resample if requested\n",
    "        if resample_freq:\n",
    "            if agg_func is None:\n",
    "                raise ValueError(\"agg_func must be provided when resampling.\")\n",
    "            df = df.resample(resample_freq).agg(agg_func)\n",
    "            \n",
    "        data_dict[ticker] = df\n",
    "        \n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c842d7-d7e3-4a95-8175-cc45ca9939a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckdb in c:\\users\\mangl\\desktop\\capstone\\venv\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\mangl\\desktop\\capstone\\venv\\lib\\site-packages (21.0.0)\n",
      "Requirement already satisfied: polars in c:\\users\\mangl\\desktop\\capstone\\venv\\lib\\site-packages (1.32.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install duckdb pyarrow polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ad62d54-4f28-4f16-bcb7-3771c230cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pyarrow\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5a4c51c-3957-4704-878d-c3a82981430b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cca4b82b0c48dcb3e17411b5c62138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 104251560 volume records\n"
     ]
    }
   ],
   "source": [
    "volume_data = duckdb.sql(\"\"\"\n",
    "    SELECT \n",
    "        \"<ticker>\" AS ticker,\n",
    "        \"<volume>\" AS volume\n",
    "    FROM read_csv('../downloaded_files/Cash Data * 2021/*.csv')\n",
    "\"\"\") \n",
    "\n",
    "print(f\"✅ Loaded {len(volume_data)} volume records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb3d3c-d103-447b-9093-bc91d74c6c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7abbaa-51b0-499f-921c-3aa2b0e22315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Global set to store illiquid tickers\n",
    "ILLIQUID_TICKERS = set()\n",
    "\n",
    "def get_stock_data(tickers, impute=False, DATA_DIR=None):\n",
    "    \"\"\"\n",
    "    Mock function to retrieve stock data for a list of tickers.\n",
    "    Replace with actual implementation.\n",
    "    Returns a dictionary of {ticker: DataFrame}.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for ticker in tickers:\n",
    "        file_path = Path(DATA_DIR) / f\"{ticker}.csv\"\n",
    "        if file_path.exists():\n",
    "            df = pd.read_csv(file_path)\n",
    "            result[ticker] = df\n",
    "    return result\n",
    "\n",
    "def identify_illiquid_tickers(ticker_dfs, volume_threshold=1000, \n",
    "                            zero_volume_days_threshold=0.15, min_nonzero_volume_threshold=100):\n",
    "    illiquid = set()\n",
    "    for ticker, df in ticker_dfs.items():\n",
    "        if 'volume' not in df.columns:\n",
    "            illiquid.add(ticker)\n",
    "            continue\n",
    "        avg_volume = df['volume'].mean()\n",
    "        fraction_zero_days = (df['volume'] == 0).sum() / len(df) if len(df) > 0 else 1\n",
    "        nonzero_days = df[df['volume'] > 0]\n",
    "        avg_nonzero_volume = nonzero_days['volume'].mean() if len(nonzero_days) > 0 else 0\n",
    "        if (avg_volume < volume_threshold or \n",
    "            fraction_zero_days > zero_volume_days_threshold or \n",
    "            avg_nonzero_volume < min_nonzero_volume_threshold):\n",
    "            illiquid.add(ticker)\n",
    "    return illiquid\n",
    "\n",
    "def process_stock_data(columns, data_dir_base=\"../downloaded_files\", year=2021,\n",
    "                      volume_threshold=1000, zero_volume_days_threshold=0.15,\n",
    "                      min_nonzero_volume_threshold=100):\n",
    "    months = list(calendar.month_name)[1:]  # Skip empty string at index 0\n",
    "\n",
    "    def process_ticker(ticker, data_dir):\n",
    "        if ticker in ILLIQUID_TICKERS:\n",
    "            return None\n",
    "        try:\n",
    "            df = get_stock_data([ticker], impute=False, DATA_DIR=data_dir)[ticker]\n",
    "            return ticker, df\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def process_month(month):\n",
    "        DATA_DIR = Path(f\"{data_dir_base}/Cash Data {month} {year}\")\n",
    "        if not DATA_DIR.exists():\n",
    "            return {}\n",
    "        tickers = [f.rsplit('.', 1)[0] for f in os.listdir(DATA_DIR) if f.endswith('.csv')]\n",
    "        tickers = list(set(tickers) - ILLIQUID_TICKERS)\n",
    "        results = Parallel(n_jobs=-1)(\n",
    "            delayed(process_ticker)(ticker, DATA_DIR) for ticker in tickers\n",
    "        )\n",
    "        return {ticker: df for result in results if result is not None for ticker, df in [result]}\n",
    "\n",
    "    # Parallelize months\n",
    "    month_dicts = Parallel(n_jobs=-1)(\n",
    "        delayed(process_month)(month) for month in months\n",
    "    )\n",
    "\n",
    "    # Merge per-ticker\n",
    "    dfs = defaultdict(list)\n",
    "    for month_dict in month_dicts:\n",
    "        for ticker, df in month_dict.items():\n",
    "            dfs[ticker].append(df)\n",
    "\n",
    "    ticker_dfs = {ticker: pd.concat(df_list, ignore_index=True)\n",
    "                  for ticker, df_list in dfs.items()}\n",
    "\n",
    "    # Identify illiquid tickers\n",
    "    new_illiquid = identify_illiquid_tickers(\n",
    "        ticker_dfs, volume_threshold, zero_volume_days_threshold, min_nonzero_volume_threshold\n",
    "    )\n",
    "    ILLIQUID_TICKERS.update(new_illiquid)\n",
    "\n",
    "    # Return only liquid tickers\n",
    "    liquid_dfs = {ticker: df for ticker, df in ticker_dfs.items() if ticker not in ILLIQUID_TICKERS}\n",
    "    return liquid_dfs\n",
    "\n",
    "# Example usage\n",
    "columns = [\"volume\"]\n",
    "result = process_stock_data(\n",
    "    columns,\n",
    "    volume_threshold=1000,\n",
    "    zero_volume_days_threshold=0.15,\n",
    "    min_nonzero_volume_threshold=100\n",
    ")\n",
    "print(f\"Processed {len(result)} liquid tickers\")\n",
    "print(f\"Illiquid tickers: {ILLIQUID_TICKERS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea7a99-4f6c-4cc1-9304-887c4a2e7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed147b9a-f08a-479b-90d9-16e55fa55901",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_volume_tickers = {ticker  for ticker, df in dfs.items() if df.volume.sum() == 0}\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651ccab-6f40-4c9e-b45e-1ec3804191e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {ticker: df for ticker, df in dfs.items() if ticker not in ticker zero_volume_tickers}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10c52a-e0d3-4b78-a530-02ff90de1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = get_stock_data(\n",
    "#     tickers,\n",
    "#     impute=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4279aa-081a-4465-ab2c-44a56a148916",
   "metadata": {},
   "source": [
    "Mean trading volume for all the stocks for the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b83a6-61e2-4d0b-909a-5aa7005a1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mean_monthly_traded_volume = sum(df.volume.mean() for _, df in dfs.items())/len(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d02de5-c405-4a5f-9080-340e2e9555e8",
   "metadata": {},
   "source": [
    " Here we find the stocks whichh have high trading mean trading volumes for atleast half the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c0ca3-f258-4766-bea0-afb97c7c1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate high volume day count for each stock\n",
    "high_volume_day_counts = {\n",
    "    s: (df.groupby(df.index.date)['volume'].mean() > overall_mean_monthly_traded_volume).sum().item()\n",
    "    for s, df in dfs.items()\n",
    "}\n",
    "\n",
    "# Histogram \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(high_volume_day_counts.values(), bins=10, edgecolor='black')\n",
    "plt.xlabel('Number of High-Volume Days')\n",
    "plt.ylabel('Number of Stocks')\n",
    "plt.title('Distribution of High-Volume Days Across Stocks')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d6d95-055b-4100-8fde-09aa81820118",
   "metadata": {},
   "source": [
    "We observe that for more than half of January, there was sufficient liquidity in the market for some of the stocks. A huge number of stocks have been illiquid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70cb61-63de-422e-adcc-2ff1891b5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "liquid_stocks = [ s for s, v in high_volume_day_counts.items() if v > 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880ff59-6ce1-42b3-8352-3b326a4d5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "def calculate_half_life(price_series):\n",
    "    \"\"\"\n",
    "    Calculates the half-life of mean reversion for a price series using linear regression.\n",
    "    \n",
    "    Reference:\n",
    "        Chan, E. P. (2013). Algorithmic Trading: Winning Strategies and Their Rationale. \n",
    "        John Wiley & Sons.\n",
    "    \"\"\"\n",
    "    delta_p = price_series.diff().dropna()\n",
    "    p_lag = price_series.shift(1).dropna()\n",
    "    \n",
    "    # Align indices explicitly - keep only indices that appear in both\n",
    "    common_idx = delta_p.index.intersection(p_lag.index)\n",
    "    delta_p = delta_p.loc[common_idx]\n",
    "    p_lag = p_lag.loc[common_idx]\n",
    "    \n",
    "    model = OLS(delta_p.values, add_constant(p_lag.values))\n",
    "    res = model.fit()\n",
    "    beta = res.params[1]\n",
    "    halflife = -np.log(2) / beta if beta < 0 else np.inf\n",
    "    return halflife\n",
    "\n",
    "def hurst_exponent(ts):\n",
    "    lags = range(2, 100)\n",
    "    tau = [np.sqrt(np.std(np.subtract(ts[lag:], ts[:-lag]))) for lag in lags]\n",
    "    poly = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "    return poly[0] * 2.0\n",
    "\n",
    "def mean_reversion_vector(price_series):\n",
    "    price_series = price_series.dropna()\n",
    "    \n",
    "    adf_stat = adfuller(price_series)[0]\n",
    "    half_life = calculate_half_life(price_series)\n",
    "    hurst = hurst_exponent(price_series.values)\n",
    "    \n",
    "    returns = price_series.pct_change().dropna()\n",
    "    vol = returns.std()\n",
    "    \n",
    "    return np.array([adf_stat, half_life, hurst, vol])\n",
    "\n",
    "# Now your embedding extraction code can stay the same:\n",
    "embedding_list = []\n",
    "valid_stocks = []\n",
    "\n",
    "# for ticker in liquid_stocks:\n",
    "#     df = dfs[ticker]\n",
    "#     price_series = df['close']\n",
    "    \n",
    "#     try:\n",
    "#         vec = mean_reversion_vector(price_series)\n",
    "#         if np.all(np.isfinite(vec)):\n",
    "#             embedding_list.append(vec)\n",
    "#             valid_stocks.append(ticker)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed for {ticker}: {e}\")\n",
    "\n",
    "# embeddings = np.vstack(embedding_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155e8cf-75e4-44c2-93d5-0fecca037bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def process_ticker(ticker, price_series):\n",
    "    \"\"\"Compute mean-reversion vector for one ticker\"\"\"\n",
    "    try:\n",
    "        vec = mean_reversion_vector(price_series)\n",
    "        if np.all(np.isfinite(vec)):\n",
    "            return ticker, vec\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {ticker}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Prepare inputs first (avoid passing big dfs dict to workers)\n",
    "inputs = [(ticker, dfs[ticker]['close']) for ticker in liquid_stocks]\n",
    "\n",
    "results = Parallel(n_jobs=8, backend=\"loky\")(  # try 8 workers first\n",
    "    delayed(process_ticker)(ticker, price_series) \n",
    "    for ticker, price_series in inputs\n",
    ")\n",
    "\n",
    "valid_results = [r for r in results if r is not None]\n",
    "valid_stocks, embedding_list = zip(*valid_results)\n",
    "embeddings = np.vstack(embedding_list)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece7f3f-9ad8-455d-89f4-0dd61f76412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Cosine similarity matrix\n",
    "cos_sim = cosine_similarity(embeddings)\n",
    "\n",
    "# Cluster \n",
    "cluster_model = AgglomerativeClustering(n_clusters=20, linkage='average')\n",
    "\n",
    "distances = 1 - cos_sim\n",
    "\n",
    "labels = cluster_model.fit_predict(distances)\n",
    "\n",
    "# Now labels[i] gives cluster of valid_stocks[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07131d4-5814-4d0f-88cc-8c597b102a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbb95ba-158d-4dd2-9ac4-6ac777a479eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import umap.umap_ as umap \n",
    "\n",
    "reducer = umap.plot.interactive(min_dist =0.002, n_components=6, n_neighbors=30)\n",
    "embeddings_umap = reducer.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(embeddings_umap[:, 0], embeddings_umap[:, 1], c=labels, cmap='tab10', s=50, alpha=0.8)\n",
    "plt.title(\"Stocks clustered by mean-reversion indicators (UMAP 2D projection)\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.colorbar(scatter, label='Cluster label')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932f415-ff21-4223-ab5f-fdafcb061c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "clustered_tickers = defaultdict(list)\n",
    "for ticker, label in zip(tickers, labels):\n",
    "    clustered_tickers[label].append(ticker)\n",
    "\n",
    "for label in sorted(clustered_tickers):\n",
    "    print(f\"\\nCluster {label} ({len(clustered_tickers[label])} tickers):\")\n",
    "    print(\", \".join(clustered_tickers[label]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde930b8-a73f-496c-a822-849292629433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_pair(label, ticker1, ticker2):\n",
    "    try:\n",
    "        s1 = dfs[ticker1]['close']\n",
    "        s2 = dfs[ticker2]['close']\n",
    "        joined = pd.concat([s1, s2], axis=1, join='inner').dropna()\n",
    "        if len(joined) < 50:\n",
    "            return None\n",
    "        s1_aligned = joined.iloc[:, 0]\n",
    "        s2_aligned = joined.iloc[:, 1]\n",
    "        score, pvalue, _ = coint(s1_aligned, s2_aligned)\n",
    "        return {\n",
    "            'cluster': label,\n",
    "            'pair': (ticker1, ticker2),\n",
    "            'cointegration_pvalue': pvalue\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "all_pairs = []\n",
    "for label, ticker_list in clustered_tickers.items():\n",
    "    if 2 <= len(ticker_list) <= 10:\n",
    "        for ticker1, ticker2 in combinations(ticker_list, 2):\n",
    "            all_pairs.append((label, ticker1, ticker2))\n",
    "pair_results = []\n",
    "for label, ticker1, ticker2 in all_pairs:\n",
    "    result = evaluate_pair(label, ticker1, ticker2)\n",
    "    if result is not None:\n",
    "        pair_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780da69-7a41-42d1-b35f-55bb53fd6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "for result in pair_results:\n",
    "    t1, t2 = result['pair']\n",
    "    s1 = dfs[t1]['close']\n",
    "    s2 = dfs[t2]['close']\n",
    "    joined = pd.concat([s1, s2], axis=1, join='inner').dropna()\n",
    "    y = joined.iloc[:,0].values\n",
    "    x = joined.iloc[:,1].values\n",
    "    beta = OLS(y, add_constant(x)).fit().params[1]\n",
    "    spread = y - beta * x\n",
    "    adf_stat, adf_pvalue, *_ = adfuller(spread)\n",
    "    result['spread_adf_stat'] = adf_stat\n",
    "    result['spread_adf_pvalue'] = adf_pvalue\n",
    "    result['spread_std'] = np.std(spread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56db14bc-adbf-44e7-9ec8-ed51ef6742e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_pairs = [\n",
    "    r for r in pair_results\n",
    "    if r['cointegration_pvalue'] < 0.05 and r['spread_adf_pvalue'] < 0.05\n",
    "]\n",
    "#  sort by (1) p-value, (2) spread_std, (3) cluster size\n",
    "good_pairs = sorted(good_pairs, key=lambda r: (r['cointegration_pvalue'], -r['spread_std']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec84d08-98eb-4a2c-8dac-727375d7bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_report = pd.DataFrame(good_pairs)\n",
    "print(df_report[['cluster', 'pair', 'cointegration_pvalue', 'spread_adf_pvalue', 'spread_std']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fcc9a-7de1-42c9-9a35-78977ac9e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = good_pairs[0]['pair']\n",
    "s1 = dfs[pair[0]]['close']\n",
    "s2 = dfs[pair[1]]['close']\n",
    "joined = pd.concat([s1, s2], axis=1, join='inner').dropna()\n",
    "y = joined.iloc[:,0].values\n",
    "x = joined.iloc[:,1].values\n",
    "beta = OLS(y, add_constant(x)).fit().params[1]\n",
    "spread = y - beta * x\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(joined.index, spread)\n",
    "plt.title(f\"Spread between {pair[0]} and {pair[1]}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Spread\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b48c09-63b1-4576-b78f-f77615fba892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09019601-6f20-4ae5-a9e1-d7e4951c8b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
