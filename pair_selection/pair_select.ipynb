{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f6e434-f76b-40b9-ad5e-0cfc73bd2ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in c:\\users\\mangl\\desktop\\capstone\\venv\\lib\\site-packages (0.61.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\mangl\\desktop\\capstone\\venv\\lib\\site-packages (from numba) (0.44.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.24 in c:\\users\\mangl\\desktop\\capstone\\venv\\lib\\site-packages (from numba) (2.2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc87b17-1206-4ccc-a7c4-bb007d217974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import sys \n",
    "from typing import List, Optional, Callable, Dict, Union\n",
    "from collections import defaultdict\n",
    "import calendar\n",
    "from joblib import Parallel, delayed\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Union, Optional, Callable\n",
    "import polars as pl\n",
    "from IPython.core.debugger import set_trace \n",
    "from numba import njit\n",
    "# This code was developed with the assistance of multiple AI agents (e.g., Perplexity, August 2025).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310d41cc-6991-45c0-a880-2d038550b758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\mangl\\\\Desktop\\\\capstone\\\\pair_selection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6d6597c-bc12-4caf-b831-2bc7d87a8217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../downloaded_files/Cash Data January 2021/.CNX100.csv'),\n",
       " WindowsPath('../downloaded_files/Cash Data January 2021/.CNXIT.csv'),\n",
       " WindowsPath('../downloaded_files/Cash Data January 2021/.NSEBANK.csv'),\n",
       " WindowsPath('../downloaded_files/Cash Data January 2021/.NSEI.csv'),\n",
       " WindowsPath('../downloaded_files/Cash Data January 2021/20MICRONS.csv'),\n",
       " WindowsPath('../downloaded_files/Cash Data January 2021/21STCENMGM.csv'),\n",
       " WindowsPath('../downloaded_files/Cash Data January 2021/3IINFOTECH.csv'),\n",
       " WindowsPath('../downloaded_files/Cash Data January 2021/3MINDIA.csv'),\n",
       " WindowsPath('../downloaded_files/Cash Data January 2021/3PLAND.csv'),\n",
       " WindowsPath('../downloaded_files/Cash Data January 2021/5PAISA.csv')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Path(f\"../downloaded_files/Cash Data January 2021\").iterdir())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b7228a9-4779-403a-a105-9adc18718794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;ticker&gt;</th>\n",
       "      <th>&lt;date&gt;</th>\n",
       "      <th>&lt;time&gt;</th>\n",
       "      <th>&lt;open&gt;</th>\n",
       "      <th>&lt;high&gt;</th>\n",
       "      <th>&lt;low&gt;</th>\n",
       "      <th>&lt;close&gt;</th>\n",
       "      <th>&lt;volume&gt;</th>\n",
       "      <th>&lt;o/i&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>09:45:00</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>09:57:00</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>09:59:00</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>10:10:00</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>10:13:00</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>10.65</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/29/2021</td>\n",
       "      <td>13:27:00</td>\n",
       "      <td>11.10</td>\n",
       "      <td>11.10</td>\n",
       "      <td>11.10</td>\n",
       "      <td>11.10</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/29/2021</td>\n",
       "      <td>14:47:00</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/29/2021</td>\n",
       "      <td>15:26:00</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/29/2021</td>\n",
       "      <td>15:28:00</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>21STCENMGM</td>\n",
       "      <td>01/29/2021</td>\n",
       "      <td>15:49:00</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>11.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>526 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       <ticker>      <date>    <time>  <open>  <high>  <low>  <close>  \\\n",
       "0    21STCENMGM  01/01/2021  09:45:00   10.65   10.65  10.65    10.65   \n",
       "1    21STCENMGM  01/01/2021  09:57:00   10.65   10.65  10.65    10.65   \n",
       "2    21STCENMGM  01/01/2021  09:59:00   10.65   10.65  10.65    10.65   \n",
       "3    21STCENMGM  01/01/2021  10:10:00   10.65   10.65  10.65    10.65   \n",
       "4    21STCENMGM  01/01/2021  10:13:00   10.65   10.65  10.65    10.65   \n",
       "..          ...         ...       ...     ...     ...    ...      ...   \n",
       "521  21STCENMGM  01/29/2021  13:27:00   11.10   11.10  11.10    11.10   \n",
       "522  21STCENMGM  01/29/2021  14:47:00   11.15   11.15  11.15    11.15   \n",
       "523  21STCENMGM  01/29/2021  15:26:00   11.15   11.15  11.15    11.15   \n",
       "524  21STCENMGM  01/29/2021  15:28:00   11.15   11.15  11.15    11.15   \n",
       "525  21STCENMGM  01/29/2021  15:49:00   11.15   11.15  11.15    11.15   \n",
       "\n",
       "     <volume>  <o/i>   \n",
       "0          11       0  \n",
       "1          35       0  \n",
       "2          90       0  \n",
       "3          10       0  \n",
       "4         139       0  \n",
       "..        ...     ...  \n",
       "521       299       0  \n",
       "522         3       0  \n",
       "523        20       0  \n",
       "524         8       0  \n",
       "525         1       0  \n",
       "\n",
       "[526 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(DATA_DIR/'21STCENMGM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a6f1794-2b25-4f38-a549-ec9ece9000f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.CNX100',\n",
       " '.CNXIT',\n",
       " '.NSEBANK',\n",
       " '.NSEI',\n",
       " '20MICRONS',\n",
       " '21STCENMGM',\n",
       " '3IINFOTECH',\n",
       " '3MINDIA',\n",
       " '3PLAND',\n",
       " '5PAISA']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = [i.rsplit(\".\", maxsplit=1)[0] for i in os.listdir(DATA_DIR)]\n",
    "tickers[:10] # few available tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6feee945-6512-4691-9b4f-17ef5c72a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def get_stock_data(\n",
    "    tickers: List[str],\n",
    "    start: Optional[Union[str, datetime]] = None,\n",
    "    end: Optional[Union[str, datetime]] = None,\n",
    "    agg_func: Optional[Union[str, Callable]] = None,\n",
    "    resample_freq: Optional[str] = None,\n",
    "    select_columns: Optional[List[str]] = None,\n",
    "    impute: bool = True,\n",
    "    DATA_DIR: Optional[Union[str, Path]] = None\n",
    ") -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load and process intraday stock data from CSV files using Polars.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tickers : List[str]\n",
    "        List of stock ticker symbols (without file extensions).\n",
    "    start : str or datetime, optional\n",
    "        Start datetime (format: \"YYYY-MM-DD\" or datetime object).\n",
    "    end : str or datetime, optional\n",
    "        End datetime (format: \"YYYY-MM-DD\" or datetime object).\n",
    "    agg_func : str or callable, optional\n",
    "        Aggregation function for resampling (e.g., \"mean\", \"sum\").\n",
    "    resample_freq : str, optional\n",
    "        Resample frequency (e.g., \"5m\", \"15m\").\n",
    "    columns : List[str], optional\n",
    "        List of columns to retain (e.g., [\"open_price\", \"close_price\"]).\n",
    "    impute : bool, default True\n",
    "        Whether to impute missing data using forward fill.\n",
    "    DATA_DIR : str or Path, optional\n",
    "        Directory containing the CSV files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, pl.DataFrame]\n",
    "        Dictionary mapping each ticker to its processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Convert DATA_DIR to Path\n",
    "    DATA_DIR = Path(DATA_DIR)\n",
    "\n",
    "    rev_cols_map = {\n",
    "        \"ticker\": \"<ticker>\",\n",
    "        \"date\": \"<date>\",\n",
    "        \"time\": \"<time>\",\n",
    "        \"open_price\": \"<open>\",\n",
    "        \"high_price\": \"<high>\",\n",
    "        \"low_price\": \"<low>\",\n",
    "        \"close_price\": \"<close>\",\n",
    "        \"volume\": \"<volume>\",\n",
    "        \"open_interest\": \"<o/i> \"\n",
    "    }\n",
    "    new_columns, columns = zip(*rev_cols_map.items())\n",
    "    if select_columns:    \n",
    "        new_columns = select_columns + [\"date\", \"time\"]\n",
    "        columns = [rev_cols_map[i] for i in new_columns]\n",
    "\n",
    "    data_dict = {}\n",
    "    \n",
    "    start = datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\")  \n",
    "    end = datetime.strptime(end, \"%Y-%m-%d %H:%M:%S\")  \n",
    "    \n",
    "    for ticker in tickers:\n",
    "        # Read CSV\n",
    "        df = pl.read_csv(DATA_DIR / f\"{ticker}.csv\", columns=columns).rename(self._cols_map)\n",
    "\n",
    "        # Create datetime column\n",
    "        df = df.with_columns(\n",
    "            pl.concat_str([df[\"date\"], df[\"time\"]], separator=\" \")\n",
    "            .str.strptime(pl.Datetime, format=\"%m/%d/%Y %H:%M:%S\")\n",
    "            .alias(\"datetime\")\n",
    "        ).sort(\"datetime\")\n",
    "\n",
    "        # Drop extra columns\n",
    "        df = df.drop([\"ticker\", \"date\", \"time\"])\n",
    "        df = df.set_sorted(\"datetime\")\n",
    "\n",
    "        # Impute missing values\n",
    "        if impute:\n",
    "            df = df.upsample(\n",
    "                time_column=\"datetime\",\n",
    "                every=\"1m\",\n",
    "                maintain_order=True\n",
    "            ).fill_null(strategy=\"forward\")\n",
    "        df = df.filter((pl.col(\"datetime\") >= start) & (pl.col(\"datetime\") <= end))\n",
    "\n",
    "        # Resample if requested\n",
    "        if resample_freq:\n",
    "            if not agg_func:\n",
    "                agg_func = \"mean\"\n",
    "            df = df.group_by_dynamic(\n",
    "                time_column=\"datetime\",\n",
    "                every=resample_freq,\n",
    "                closed=\"left\"\n",
    "            ).agg({col: agg_func for col in df.columns if col != \"datetime\"})\n",
    "\n",
    "        \n",
    "\n",
    "        # Keep only requested columns + datetime\n",
    "        keep_cols = [\"datetime\"] + [col for col in new_columns if col in df.columns]\n",
    "        df = df.select(keep_cols)\n",
    "\n",
    "        data_dict[ticker] = df\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57c0ee91-b2d2-437a-b36a-6d5b038ac38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Optional, Union, Dict, Callable\n",
    "import polars as pl\n",
    "import calendar\n",
    "\n",
    "\n",
    "class StockDataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tickers: List[str],\n",
    "        start: Union[str, datetime],\n",
    "        end: Union[str, datetime],\n",
    "        agg_func: Optional[Union[str, Callable]] = None,\n",
    "        resample_freq: Optional[str] = None,\n",
    "        select_columns: Optional[List[str]] = None,\n",
    "        impute: bool = True,\n",
    "        base_dir: Optional[Union[str, Path]] = \"../downloaded_files\",\n",
    "    ):\n",
    "        # Make attributes private\n",
    "        self._start = self._parse_datetime(start)\n",
    "        self._end = self._parse_datetime(end)\n",
    "        self._base_dir = Path(base_dir)\n",
    "        \n",
    "        self._tickers = tickers or self._find_available_tickers()\n",
    "        \n",
    "        self._agg_func = agg_func\n",
    "        self._resample_freq = resample_freq\n",
    "        self._select_columns = select_columns  \n",
    "        self._impute = impute\n",
    "\n",
    "        # Cache for storing loaded data\n",
    "        self._data_cache: Dict[str, pl.DataFrame] = {}\n",
    "\n",
    "        # Column mapping for renaming\n",
    "        self._rev_cols_map = {\n",
    "            \"open_interest\": \"<o/i> \",\n",
    "            \"date\": \"<date>\",\n",
    "            \"time\": \"<time>\",\n",
    "            \"open\": \"<open>\",\n",
    "            \"high\": \"<high>\",\n",
    "            \"low\": \"<low>\",\n",
    "            \"close\": \"<close>\",\n",
    "            \"volume\": \"<volume>\",\n",
    "        }\n",
    "        self._cols_map = {\n",
    "            \"<o/i> \": \"open_interest\",\n",
    "            \"<date>\": \"date\",\n",
    "            \"<time>\": \"time\",\n",
    "            \"<open>\": \"open\",\n",
    "            \"<high>\": \"high\",\n",
    "            \"<low>\": \"low\",\n",
    "            \"<close>\": \"close\",\n",
    "            \"<volume>\": \"volume\"\n",
    "        }\n",
    "        self._new_columns, self._columns = zip(*self._rev_cols_map.items())\n",
    "        if self._select_columns:\n",
    "            self._new_columns = self._select_columns + [\"date\", \"time\"]\n",
    "            self._columns = [self._rev_cols_map[c] for c in self._new_columns]\n",
    "\n",
    "        self._schema = {\n",
    "             \"date\": pl.String,\n",
    "            \"time\":pl.String,\n",
    "            \"datetime\": pl.Datetime(\"us\"),\n",
    "            \"open_price\": pl.Float64,\n",
    "            \"high_price\": pl.Float64,\n",
    "            \"low_price\": pl.Float64,\n",
    "            \"close_price\": pl.Float64,\n",
    "            \"volume\": pl.Float64,\n",
    "            \"open_interest\": pl.Float64,\n",
    "        }\n",
    "    def _find_available_tickers(self) -> List[str]:\n",
    "        \"\"\"Find all tickers present across all months by set intersection.\"\"\"\n",
    "        months = self._generate_monthly_files()\n",
    "        all_month_tickers = []\n",
    "\n",
    "        for month in months:\n",
    "            folder = self._get_month_dir(month)\n",
    "            if not folder.exists():\n",
    "                continue\n",
    "\n",
    "            tickers_in_month = {\n",
    "                f.stem for f in folder.glob(\"*.csv\") if f.is_file()\n",
    "            }\n",
    "            if tickers_in_month:\n",
    "                all_month_tickers.append(tickers_in_month)\n",
    "\n",
    "        # If no CSVs found at all\n",
    "        if not all_month_tickers:\n",
    "            raise FileNotFoundError(\n",
    "                f\"No CSV files found in any month between {self._start} and {self._end}\"\n",
    "            )\n",
    "\n",
    "        # Take intersection to get tickers present in *all* months\n",
    "        common_tickers = set.intersection(*all_month_tickers)\n",
    "        if not common_tickers:\n",
    "            raise ValueError(\n",
    "                \"No common tickers found across all months in the given date range.\"\n",
    "            )\n",
    "\n",
    "        return sorted(common_tickers)\n",
    "    def _parse_datetime(self, dt):\n",
    "        \"\"\"Ensure datetime parsing.\"\"\"\n",
    "        if isinstance(dt, datetime):\n",
    "            return dt\n",
    "        return datetime.strptime(dt, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def _generate_monthly_files(self):\n",
    "        \"\"\"Generate list of months between start and end.\"\"\"\n",
    "        start_month = self._start.replace(day=1)\n",
    "        end_month = self._end.replace(day=1)\n",
    "\n",
    "        months = []\n",
    "        while start_month <= end_month:\n",
    "            months.append(start_month.strftime(\"%Y-%m\"))  # e.g. \"2025-01\"\n",
    "            start_month += timedelta(days=32)\n",
    "            start_month = start_month.replace(day=1)\n",
    "        return months\n",
    "\n",
    "    def _get_month_dir(self, month: str) -> Path:\n",
    "        \"\"\"\n",
    "        Construct the directory path dynamically.\n",
    "        Example: \"../downloaded_files/Cash Data January 2025\"\n",
    "        \"\"\"\n",
    "        year, month_num = month.split(\"-\")\n",
    "        month_name = calendar.month_name[int(month_num)]\n",
    "        return self._base_dir / f\"Cash Data {month_name} {year}\"\n",
    "\n",
    "    def _empty_df(self) -> pl.DataFrame:\n",
    "        \"\"\"Return an empty DataFrame with correct schema.\"\"\"\n",
    "        return pl.DataFrame(schema=self._schema)\n",
    "\n",
    "    def _load_single_file(self, ticker: str, month: str) -> pl.DataFrame:\n",
    "        \"\"\"Load and preprocess a single month's CSV for one ticker.\"\"\"\n",
    "        folder = self._get_month_dir(month)\n",
    "        file_path = folder / f\"{ticker}.csv\"\n",
    "        \n",
    "\n",
    "        # If file doesn't exist → return empty DataFrame with correct schema\n",
    "        if not file_path.exists():\n",
    "            return self._empty_df()\n",
    "\n",
    "        df = pl.read_csv(file_path, columns=self._columns)\n",
    "        df = df.rename({i:self._cols_map[i] for i in df.columns})\n",
    "        \n",
    "\n",
    "        # Create datetime column\n",
    "        df = df.with_columns(\n",
    "            pl.concat_str([df[\"date\"], df[\"time\"]], separator=\" \")\n",
    "            .str.strptime(pl.Datetime, format=\"%m/%d/%Y %H:%M:%S\")\n",
    "            .alias(\"datetime\")\n",
    "        ).sort(\"datetime\")\n",
    "\n",
    "        df = df.drop([ \"date\", \"time\"])\n",
    "        df = df.set_sorted(\"datetime\")\n",
    "\n",
    "        # Impute missing values if needed\n",
    "        if self._impute:\n",
    "            df = df.upsample(\n",
    "                time_column=\"datetime\",\n",
    "                every=\"1m\",\n",
    "                maintain_order=True\n",
    "            ).fill_null(strategy=\"forward\")\n",
    "\n",
    "        # Filter by start & end for partial month reads\n",
    "        return df.filter((pl.col(\"datetime\") >= self._start) & (pl.col(\"datetime\") <= self._end))\n",
    "\n",
    "    def _merge_monthly_data(self, ticker: str) -> pl.DataFrame:\n",
    "        \"\"\"Read multiple monthly files and combine them.\"\"\"\n",
    "        months = self._generate_monthly_files()\n",
    "        dfs = [self._load_single_file(ticker, month) for month in months]\n",
    "        # Always concatenate with schema alignment\n",
    "        df = pl.concat(dfs, how=\"vertical\") if dfs else self._empty_df()\n",
    "\n",
    "        # Resample if requested\n",
    "        if not df.is_empty() and self._resample_freq:\n",
    "            func = self._agg_func or \"mean\"\n",
    "            df = df.group_by_dynamic(\n",
    "                time_column=\"datetime\",\n",
    "                every=self._resample_freq,\n",
    "                closed=\"left\"\n",
    "            ).agg({col: func for col in df.columns if col != \"datetime\"})\n",
    "\n",
    "        # Keep only required columns\n",
    "        keep_cols = [\"datetime\"] + [col for col in self._new_columns if col in df.columns]\n",
    "        return df.select(keep_cols)\n",
    "     \n",
    "    def get_data_for_tickers(self, tickers: Optional[List[str]] = None) -> Dict[str, pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Public method: Returns processed data for one or more tickers.\n",
    "        Uses cache when available.\n",
    "        Fully parallelized using joblib with all available CPU cores.\n",
    "        \"\"\"\n",
    "        if not tickers:\n",
    "            tickers = self._tickers\n",
    "        else:\n",
    "            tickers = [t for t in tickers if t in self._tickers]\n",
    "\n",
    "        if not tickers:\n",
    "            raise ValueError(\"No valid tickers provided or found.\")\n",
    "\n",
    "        # Use all available cores\n",
    "        n_jobs = os.cpu_count() or 1\n",
    "\n",
    "        # Run in parallel for all tickers\n",
    "        results_list = Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
    "            delayed(self._load_or_get_from_cache)(ticker) for ticker in tickers\n",
    "        )\n",
    "\n",
    "        # Combine ticker names with their data\n",
    "        return dict(zip(tickers, results_list))\n",
    "\n",
    "    def _load_or_get_from_cache(self, ticker: str) -> pl.DataFrame:\n",
    "        \"\"\"Helper to either fetch from cache or load fresh.\"\"\"\n",
    "        if ticker not in self._data_cache:\n",
    "            self._data_cache[ticker] = self._merge_monthly_data(ticker)\n",
    "        return self._data_cache[ticker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58e12283-2c3b-4af8-b6b5-84b83f736bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = StockDataLoader(\n",
    "    tickers=[],\n",
    "    start=\"2021-12-02 09:15:00\",\n",
    "    end=\"2022-03-02 15:30:00\",\n",
    "    # resample_freq=\"5m\",\n",
    "    select_columns=[\"volume\"],\n",
    "    base_dir=\"../downloaded_files\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37d064a5-f282-4ca5-a33a-17463074375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = loader.get_data_for_tickers();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b990dcf-4a92-49ce-99b3-76faf44f05a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_dict = get_stock_data(\n",
    "#     tickers=tickers,\n",
    "#     start=\"2021-01-02 9:15:00\",\n",
    "#     end=\"2021-01-05 10:40:00\",\n",
    "#     agg_func=\"mean\",\n",
    "#     # resample_freq=\"5m\",\n",
    "#     # impute=True,\n",
    "#     DATA_DIR=DATA_DIR\n",
    "# )\n",
    "# # data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa5ee3e3-fd69-4cec-b6a9-a02b480cbffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (122_487, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>datetime</th><th>volume</th></tr><tr><td>datetime[μs]</td><td>i64</td></tr></thead><tbody><tr><td>2021-12-02 09:15:00</td><td>68</td></tr><tr><td>2021-12-02 09:16:00</td><td>68</td></tr><tr><td>2021-12-02 09:17:00</td><td>1</td></tr><tr><td>2021-12-02 09:18:00</td><td>1</td></tr><tr><td>2021-12-02 09:19:00</td><td>1</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2022-03-02 15:26:00</td><td>6</td></tr><tr><td>2022-03-02 15:27:00</td><td>1</td></tr><tr><td>2022-03-02 15:28:00</td><td>32</td></tr><tr><td>2022-03-02 15:29:00</td><td>31</td></tr><tr><td>2022-03-02 15:30:00</td><td>31</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (122_487, 2)\n",
       "┌─────────────────────┬────────┐\n",
       "│ datetime            ┆ volume │\n",
       "│ ---                 ┆ ---    │\n",
       "│ datetime[μs]        ┆ i64    │\n",
       "╞═════════════════════╪════════╡\n",
       "│ 2021-12-02 09:15:00 ┆ 68     │\n",
       "│ 2021-12-02 09:16:00 ┆ 68     │\n",
       "│ 2021-12-02 09:17:00 ┆ 1      │\n",
       "│ 2021-12-02 09:18:00 ┆ 1      │\n",
       "│ 2021-12-02 09:19:00 ┆ 1      │\n",
       "│ …                   ┆ …      │\n",
       "│ 2022-03-02 15:26:00 ┆ 6      │\n",
       "│ 2022-03-02 15:27:00 ┆ 1      │\n",
       "│ 2022-03-02 15:28:00 ┆ 32     │\n",
       "│ 2022-03-02 15:29:00 ┆ 31     │\n",
       "│ 2022-03-02 15:30:00 ┆ 31     │\n",
       "└─────────────────────┴────────┘"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[\"ADORWELD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92231c99-d96f-46e1-9619-5604d8e7c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ipdb\n",
    "class StockDataReader:\n",
    "    COLS_MAP = {\n",
    "        \"<ticker>\": \"ticker\",\n",
    "        \"<date>\": \"date\",\n",
    "        \"<time>\": \"time\",\n",
    "        \"<open>\": \"open_price\",\n",
    "        \"<high>\": \"high_price\",\n",
    "        \"<low>\": \"low_price\",\n",
    "        \"<close>\": \"close_price\",\n",
    "        \"<volume>\": \"volume\",\n",
    "        \"<o/i>\": \"open_interest\"\n",
    "    }\n",
    "\n",
    "    REV_COLS_MAP = {v: k for k, v in COLS_MAP.items()}\n",
    "\n",
    "    def __init__(self, data_dir: Path):\n",
    "        self.data_dir = Path(data_dir)\n",
    "\n",
    "    def _get_file_paths(self, ticker: str, start: pd.Timestamp, end: pd.Timestamp):\n",
    "        \"\"\"\n",
    "        Generate all possible file paths for a ticker between start and end dates.\n",
    "        Expects folder names like \"Cash Data January 2021\".\n",
    "        \"\"\"\n",
    "        file_paths = []\n",
    "        \n",
    "        # Normalize start and end to the first of their months\n",
    "        start = start.normalize().replace(day=1)\n",
    "        end = end.normalize().replace(day=1)\n",
    "        \n",
    "        # Iterate over each month start in range\n",
    "        for ts in pd.date_range(start, end, freq=\"MS\"):\n",
    "            month_name = calendar.month_name[ts.month]\n",
    "            year = ts.year\n",
    "            folder = self.data_dir / f\"Cash Data {month_name} {year}\"\n",
    "            \n",
    "            file_path = folder / f\"{ticker}.csv\"\n",
    "            if file_path.exists():\n",
    "                file_paths.append(file_path)\n",
    "        \n",
    "        return file_paths\n",
    "\n",
    "\n",
    "    def _read_single_ticker(\n",
    "        self, ticker: str, start=None, end=None, agg_func=None,\n",
    "        resample_freq=None, columns=None, impute=True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Load and process a single ticker CSV into a cleaned DataFrame\"\"\"\n",
    "        start = pd.to_datetime(start) \n",
    "        end = pd.to_datetime(end)  \n",
    "        file_paths = self._get_file_paths(ticker, start, end)\n",
    "        \n",
    "        dfs = []\n",
    "        for file_path in file_paths:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.rename(columns=self.COLS_MAP)\n",
    "            dfs.append(df)\n",
    "\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Create datetime column\n",
    "        df[\"datetime\"] = pd.to_datetime(\n",
    "            df[\"date\"] + \" \" + df[\"time\"], format=\"%m/%d/%Y %H:%M:%S\"\n",
    "        )\n",
    "        df = df.drop(columns=[\"ticker\", \"date\", \"time\"], errors=\"ignore\")\n",
    "        df = df.set_index(\"datetime\").sort_index()\n",
    "\n",
    "        if resample_freq:\n",
    "            impute = True\n",
    "\n",
    "        # Impute missing data\n",
    "        if impute and not df.empty:\n",
    "            full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq=\"1min\")\n",
    "            df = df.reindex(full_index)\n",
    "            for col in df.columns:\n",
    "                df[col] = df[col].fillna(df[col].expanding().median())\n",
    "\n",
    "\n",
    "        df = df[(df.index >= start) & (df.index <= end)]\n",
    "\n",
    "        # Resample if requested\n",
    "        if resample_freq:\n",
    "            if agg_func is None:\n",
    "                raise ValueError(\"agg_func must be provided when resampling.\")\n",
    "            df = df.resample(resample_freq).agg(agg_func)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_stock_data(\n",
    "        self, tickers, start=None, end=None, agg_func=None,\n",
    "        resample_freq=None, columns=None, impute=True\n",
    "    ) -> dict: \n",
    "        \"\"\"Load and process multiple tickers into a dict of DataFrames\"\"\"\n",
    "        data_dict = {}\n",
    "        for ticker in tickers:\n",
    "            df = self._read_single_ticker(\n",
    "                ticker,\n",
    "                start=start,\n",
    "                end=end,\n",
    "                agg_func=agg_func,\n",
    "                resample_freq=resample_freq,\n",
    "                columns=columns,\n",
    "                impute=impute,\n",
    "            )\n",
    "            data_dict[ticker] = df\n",
    "        return data_dict\n",
    "\n",
    "    def get_stock_data(\n",
    "        self, tickers, start=None, end=None, agg_func=None,\n",
    "        resample_freq=None, columns=None, impute=True, n_jobs=-1\n",
    "    ) -> dict:\n",
    "        \"\"\"Load and process multiple tickers into a dict of DataFrames (parallelized).\"\"\"\n",
    "\n",
    "        def process_one(ticker):\n",
    "            try:\n",
    "                df = self._read_single_ticker(\n",
    "                    ticker,\n",
    "                    start=start,\n",
    "                    end=end,\n",
    "                    agg_func=agg_func,\n",
    "                    resample_freq=resample_freq,\n",
    "                    columns=columns,\n",
    "                    impute=impute,\n",
    "                )\n",
    "                return ticker, df\n",
    "            except Exception as e:\n",
    "                # you can log/print here if debugging\n",
    "                return ticker, None\n",
    "\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(process_one)(ticker) for ticker in tickers\n",
    "        )\n",
    "\n",
    "        # collect only valid DataFrames\n",
    "        data_dict = {ticker: df for ticker, df in results if df is not None}\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc9be25-f825-4627-8ffd-145134c2f1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6460d36-7822-4830-9064-9fedf3af9345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reader = StockDataReader(f\"../downloaded_files\")\n",
    "\n",
    "# # Read 1-minute data for AAPL, impute missing minutes\n",
    "# df_dict = reader.get_stock_data(\n",
    "#     [\"RELIANCE\", \"3MINDIA\"], \n",
    "#     start=\"2021-10-05 09:15\", \n",
    "#     end=\"2022-01-05 15:30\", \n",
    "#     columns=[\"date\", \"time\", \"close\", \"volume\"]\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7abbaa-51b0-499f-921c-3aa2b0e22315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import calendar\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "ILLIQUID_TICKERS = set()\n",
    "PICKLE_FILE = Path(\"illiquid_tickers.pkl\")\n",
    "\n",
    "\n",
    "def load_illiquid_set():\n",
    "    global ILLIQUID_TICKERS\n",
    "    if PICKLE_FILE.exists():\n",
    "        with open(PICKLE_FILE, \"rb\") as f:\n",
    "            ILLIQUID_TICKERS = pickle.load(f)\n",
    "    else:\n",
    "        ILLIQUID_TICKERS = set()\n",
    "\n",
    "\n",
    "def save_illiquid_set():\n",
    "    with open(PICKLE_FILE, \"wb\") as f:\n",
    "        pickle.dump(ILLIQUID_TICKERS, f)\n",
    "\n",
    "\n",
    "def compute_liquidity_metrics(df: pd.DataFrame, ticker: str, lookback_days: int = 20,\n",
    "                              vol_threshold: int = 1000, zero_ratio_threshold: float = 0.15,\n",
    "                              amihud_threshold: float = 0.0001) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute liquidity metrics and return a NEW DataFrame with prefixed columns.\n",
    "    \"\"\"\n",
    "    metrics_df = df.copy().sort_index()\n",
    "\n",
    "    # Rolling and exponential volume\n",
    "    metrics_df['liquid_rolling_vol'] = metrics_df['volume'].rolling(lookback_days).mean()\n",
    "    metrics_df['liquid_ewm_vol'] = metrics_df['volume'].ewm(span=lookback_days, adjust=False).mean()\n",
    "\n",
    "    # Zero-volume ratio\n",
    "    metrics_df['liquid_zero_vol_ratio'] = (metrics_df['volume'] == 0).rolling(lookback_days).mean()\n",
    "\n",
    "    # Amihud Illiquidity\n",
    "    if {'close_price', 'open_price'}.issubset(metrics_df.columns):\n",
    "        metrics_df['return'] = metrics_df['close_price'].pct_change()\n",
    "        metrics_df['liquid_amihud'] = (metrics_df['return'].abs() / metrics_df['volume']).rolling(lookback_days).mean()\n",
    "\n",
    "    # ---- Determine liquidity flag ----\n",
    "    last_row = metrics_df.iloc[-1]\n",
    "    is_liquid = True\n",
    "\n",
    "    if last_row['liquid_rolling_vol'] < vol_threshold:\n",
    "        is_liquid = False\n",
    "    if last_row['liquid_zero_vol_ratio'] > zero_ratio_threshold:\n",
    "        is_liquid = False\n",
    "    if 'liquid_amihud' in metrics_df.columns and last_row['liquid_amihud'] > amihud_threshold:\n",
    "        is_liquid = False\n",
    "\n",
    "    metrics_df['liquid_is_liquid'] = is_liquid\n",
    "\n",
    "    return metrics_df, is_liquid\n",
    "def process_trade_day(trade_day, reader: StockDataReader,\n",
    "                      vol_threshold=1000, zero_ratio_threshold=0.15,\n",
    "                      amihud_threshold=0.0001, lookback_days=20):\n",
    "    \"\"\"\n",
    "    Process all tickers for a given trade day, filter illiquid stocks,\n",
    "    and return dict of liquid DataFrames.\n",
    "    \"\"\"\n",
    "    global ILLIQUID_TICKERS\n",
    "    load_illiquid_set()  # load saved set first\n",
    "\n",
    "    trade_day = pd.Timestamp(trade_day)\n",
    "\n",
    "    # Build month folder\n",
    "    month_name = calendar.month_name[trade_day.month]\n",
    "    year = trade_day.year\n",
    "    DATA_DIR = reader.data_dir / f\"Cash Data {month_name} {year}\"\n",
    "    if not DATA_DIR.exists():\n",
    "        return {}\n",
    "\n",
    "    # Collect tickers for that folder\n",
    "    tickers = [f.rsplit(\".\", 1)[0] for f in os.listdir(DATA_DIR) if f.endswith(\".csv\")]\n",
    "    tickers = list(set(tickers) - ILLIQUID_TICKERS)\n",
    "\n",
    "    # Batch load data (parallelized inside reader)\n",
    "    dfs = reader.get_stock_data(\n",
    "        tickers,\n",
    "        start=trade_day.replace(hour=9, minute=15),\n",
    "        end=trade_day.replace(hour=15, minute=30),\n",
    "        columns=[\"date\", \"time\", \"close\", \"volume\", \"open\"],\n",
    "        impute=False\n",
    "    )\n",
    "\n",
    "    liquid_dfs = {}\n",
    "    for ticker, df in dfs.items():\n",
    "        try:\n",
    "            metrics_df, is_liquid = compute_liquidity_metrics(\n",
    "                df, ticker,\n",
    "                lookback_days=lookback_days,\n",
    "                vol_threshold=vol_threshold,\n",
    "                zero_ratio_threshold=zero_ratio_threshold,\n",
    "                amihud_threshold=amihud_threshold\n",
    "            )\n",
    "\n",
    "            if is_liquid:\n",
    "                liquid_dfs[ticker] = metrics_df\n",
    "            else:\n",
    "                ILLIQUID_TICKERS.add(ticker)\n",
    "\n",
    "        except Exception:\n",
    "            # skip bad ticker\n",
    "            continue\n",
    "\n",
    "    save_illiquid_set()\n",
    "    return liquid_dfs\n",
    "\n",
    "\n",
    "reader = StockDataReader(\"../downloaded_files\")\n",
    "\n",
    "liquid_data = process_trade_day(\n",
    "    \"2021-10-05\", \n",
    "    reader,\n",
    "    vol_threshold=1000,\n",
    "    zero_ratio_threshold=0.15\n",
    ")\n",
    "\n",
    "print(\"Liquid tickers:\", list(liquid_data.keys())[:10])\n",
    "print(\"Illiquid tickers:\", list(ILLIQUID_TICKERS)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea7a99-4f6c-4cc1-9304-887c4a2e7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class AdaptiveLiquidityPredictor:\n",
    "    def __init__(self, data_reader, lookback_days=20, normalization_method='robust'):\n",
    "        self.reader = data_reader\n",
    "        self.lookback_days = lookback_days\n",
    "        self.normalization_method = normalization_method\n",
    "        \n",
    "        # EMA parameters for different time horizons\n",
    "        self.ema_params = {\n",
    "            'fast': 0.3,      # For recent 15-min updates\n",
    "            'medium': 0.1,    # For hourly patterns  \n",
    "            'slow': 0.05      # For daily/historical patterns\n",
    "        }\n",
    "        \n",
    "        # Liquidity state tracking\n",
    "        self.liquidity_states = defaultdict(dict)\n",
    "        self.historical_features = defaultdict(list)\n",
    "        \n",
    "        # Normalization components\n",
    "        self.setup_normalizers()\n",
    "        \n",
    "        # ML Models\n",
    "        self.volume_predictor = None\n",
    "        self.amihud_predictor = None\n",
    "        \n",
    "        # Model storage\n",
    "        self.model_path = Path(\"liquidity_models.pkl\")\n",
    "        self.normalization_path = Path(\"normalization_params.pkl\")\n",
    "        self.load_models()\n",
    "        self.load_normalization_params()\n",
    "\n",
    "    def setup_normalizers(self):\n",
    "        \"\"\"Initialize different types of normalizers for different data types\"\"\"\n",
    "        if self.normalization_method == 'robust':\n",
    "            # Robust to outliers - good for financial data\n",
    "            self.feature_scaler = RobustScaler()\n",
    "            self.volume_scaler = RobustScaler()\n",
    "            self.amihud_scaler = RobustScaler()\n",
    "        elif self.normalization_method == 'standard':\n",
    "            self.feature_scaler = StandardScaler()\n",
    "            self.volume_scaler = StandardScaler()\n",
    "            self.amihud_scaler = StandardScaler()\n",
    "        else:  # minmax\n",
    "            self.feature_scaler = MinMaxScaler()\n",
    "            self.volume_scaler = MinMaxScaler()\n",
    "            self.amihud_scaler = MinMaxScaler()\n",
    "        \n",
    "        # Separate scalers for different metric types\n",
    "        self.price_scaler = RobustScaler()  # For price-based metrics\n",
    "        self.ratio_scaler = MinMaxScaler()   # For ratios (already bounded)\n",
    "        \n",
    "        # Normalization parameters for EMA updates\n",
    "        self.normalization_params = {\n",
    "            'volume_stats': {'mean': 0, 'std': 1, 'q25': 0, 'q75': 1},\n",
    "            'amihud_stats': {'mean': 0, 'std': 1, 'q25': 0, 'q75': 1},\n",
    "            'price_stats': {'mean': 0, 'std': 1, 'q25': 0, 'q75': 1},\n",
    "            'initialized': False\n",
    "        }\n",
    "\n",
    "    def update_normalization_params(self, training_data):\n",
    "        \"\"\"Update normalization parameters from training data\"\"\"\n",
    "        volumes, amihuds, prices = [], [], []\n",
    "        \n",
    "        for ticker, ticker_data in training_data.items():\n",
    "            if isinstance(ticker_data, pd.DataFrame):\n",
    "                if 'volume' in ticker_data.columns:\n",
    "                    volumes.extend(ticker_data['volume'].dropna().values)\n",
    "                if 'close' in ticker_data.columns:\n",
    "                    prices.extend(ticker_data['close'].dropna().values)\n",
    "                    # Calculate returns for Amihud\n",
    "                    returns = ticker_data['close'].pct_change().dropna()\n",
    "                    volume_safe = ticker_data['volume'].replace(0, 1e-8)\n",
    "                    amihud_values = (returns.abs() / volume_safe).dropna().values\n",
    "                    amihuds.extend(amihud_values[amihud_values < np.inf])\n",
    "        \n",
    "        if volumes:\n",
    "            volumes = np.array(volumes)\n",
    "            self.normalization_params['volume_stats'] = {\n",
    "                'mean': np.mean(volumes),\n",
    "                'std': np.std(volumes) if np.std(volumes) != 0 else 1,\n",
    "                'q25': np.percentile(volumes, 25),\n",
    "                'q75': np.percentile(volumes, 75)\n",
    "            }\n",
    "        \n",
    "        if amihuds:\n",
    "            amihuds = np.array(amihuds)\n",
    "            # Handle extreme values\n",
    "            amihuds = amihuds[amihuds < np.percentile(amihuds, 95)]\n",
    "            self.normalization_params['amihud_stats'] = {\n",
    "                'mean': np.mean(amihuds),\n",
    "                'std': np.std(amihuds) if np.std(amihuds) != 0 else 1,\n",
    "                'q25': np.percentile(amihuds, 25),\n",
    "                'q75': np.percentile(amihuds, 75)\n",
    "            }\n",
    "        \n",
    "        if prices:\n",
    "            prices = np.array(prices)\n",
    "            self.normalization_params['price_stats'] = {\n",
    "                'mean': np.mean(prices),\n",
    "                'std': np.std(prices) if np.std(prices) != 0 else 1,\n",
    "                'q25': np.percentile(prices, 25),\n",
    "                'q75': np.percentile(prices, 75)\n",
    "            }\n",
    "        \n",
    "        self.normalization_params['initialized'] = True\n",
    "        self.save_normalization_params()\n",
    "\n",
    "    def normalize_metric(self, value, metric_type, method='robust'):\n",
    "        \"\"\"Normalize individual metrics using stored parameters\"\"\"\n",
    "        if not self.normalization_params['initialized']:\n",
    "            return value\n",
    "        \n",
    "        stats = self.normalization_params.get(f'{metric_type}_stats', {})\n",
    "        \n",
    "        if method == 'robust':\n",
    "            # Robust normalization using quartiles\n",
    "            iqr = stats.get('q75', 1) - stats.get('q25', 0)\n",
    "            if iqr == 0:\n",
    "                iqr = 1\n",
    "            return (value - stats.get('q25', 0)) / iqr\n",
    "        elif method == 'standard':\n",
    "            # Z-score normalization\n",
    "            std = stats.get('std', 1)\n",
    "            if std == 0:\n",
    "                std = 1\n",
    "            return (value - stats.get('mean', 0)) / std\n",
    "        else:\n",
    "            return value\n",
    "\n",
    "    def denormalize_metric(self, normalized_value, metric_type, method='robust'):\n",
    "        \"\"\"Denormalize predictions back to original scale\"\"\"\n",
    "        if not self.normalization_params['initialized']:\n",
    "            return normalized_value\n",
    "        \n",
    "        stats = self.normalization_params.get(f'{metric_type}_stats', {})\n",
    "        \n",
    "        if method == 'robust':\n",
    "            iqr = stats.get('q75', 1) - stats.get('q25', 0)\n",
    "            if iqr == 0:\n",
    "                iqr = 1\n",
    "            return normalized_value * iqr + stats.get('q25', 0)\n",
    "        elif method == 'standard':\n",
    "            std = stats.get('std', 1)\n",
    "            if std == 0:\n",
    "                std = 1\n",
    "            return normalized_value * std + stats.get('mean', 0)\n",
    "        else:\n",
    "            return normalized_value\n",
    "\n",
    "    def extract_time_features(self, timestamp):\n",
    "        \"\"\"Extract time-based features for ML models\"\"\"\n",
    "        return {\n",
    "            'hour': timestamp.hour,\n",
    "            'minute': timestamp.minute,\n",
    "            'day_of_week': timestamp.weekday(),\n",
    "            'minutes_from_open': max(0, (timestamp.hour - 9) * 60 + (timestamp.minute - 15)),\n",
    "            'minutes_to_close': max(0, (15 - timestamp.hour) * 60 + (30 - timestamp.minute)),\n",
    "            'is_opening': 1 if timestamp.hour == 9 and timestamp.minute <= 30 else 0,\n",
    "            'is_closing': 1 if timestamp.hour >= 15 else 0,\n",
    "            'is_lunch_time': 1 if 12 <= timestamp.hour <= 13 else 0\n",
    "        }\n",
    "\n",
    "    def compute_base_liquidity_metrics(self, df, ticker):\n",
    "        \"\"\"Compute base liquidity metrics from historical data with proper normalization\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            return None\n",
    "        \n",
    "        df_sorted = df.sort_index()\n",
    "        \n",
    "        # Volume metrics with outlier handling\n",
    "        volume_clean = df_sorted['volume'].clip(lower=0, upper=df_sorted['volume'].quantile(0.99))\n",
    "        rolling_vol = volume_clean.rolling(window=f'{self.lookback_days}D', min_periods=1).mean()\n",
    "        ewm_vol = volume_clean.ewm(span=self.lookback_days, adjust=False).mean()\n",
    "        vol_std = volume_clean.rolling(window=f'{self.lookback_days}D', min_periods=1).std()\n",
    "        \n",
    "        # Volume normalization\n",
    "        vol_median = volume_clean.rolling(window=f'{self.lookback_days}D', min_periods=1).median()\n",
    "        vol_mad = (volume_clean - vol_median).abs().rolling(window=f'{self.lookback_days}D', min_periods=1).median()\n",
    "        \n",
    "        # Zero volume ratio\n",
    "        zero_vol_ratio = (df_sorted['volume'] == 0).rolling(window=f'{self.lookback_days}D', min_periods=1).mean()\n",
    "        \n",
    "        # Price-based metrics with normalization\n",
    "        if 'close' in df_sorted.columns:\n",
    "            # Clean price data\n",
    "            prices = df_sorted['close'].fillna(method='ffill')\n",
    "            returns = prices.pct_change().fillna(0)\n",
    "            \n",
    "            # Winsorize extreme returns\n",
    "            return_q99 = returns.quantile(0.99)\n",
    "            return_q01 = returns.quantile(0.01)\n",
    "            returns_clean = returns.clip(lower=return_q01, upper=return_q99)\n",
    "            \n",
    "            # Amihud illiquidity with safe division\n",
    "            volume_safe = volume_clean.replace(0, volume_clean.median() * 0.01 or 1e-8)\n",
    "            amihud_raw = (returns_clean.abs() / volume_safe)\n",
    "            \n",
    "            # Robust Amihud calculation\n",
    "            amihud = amihud_raw.rolling(window=f'{self.lookback_days}D', min_periods=1).median()\n",
    "            amihud_std = amihud_raw.rolling(window=f'{self.lookback_days}D', min_periods=1).std()\n",
    "        else:\n",
    "            amihud = pd.Series(0, index=df_sorted.index)\n",
    "            amihud_std = pd.Series(0, index=df_sorted.index)\n",
    "            returns_clean = pd.Series(0, index=df_sorted.index)\n",
    "        \n",
    "        # Volatility with normalization\n",
    "        volatility = returns_clean.rolling(window=f'{self.lookback_days}D', min_periods=1).std()\n",
    "        \n",
    "        # Get latest values with safe indexing\n",
    "        metrics = {\n",
    "            'rolling_vol': rolling_vol.iloc[-1] if not rolling_vol.empty else 0,\n",
    "            'ewm_vol': ewm_vol.iloc[-1] if not ewm_vol.empty else 0,\n",
    "            'vol_std': vol_std.iloc[-1] if not vol_std.empty else 0,\n",
    "            'vol_median': vol_median.iloc[-1] if not vol_median.empty else 0,\n",
    "            'vol_mad': vol_mad.iloc[-1] if not vol_mad.empty else 1,\n",
    "            'zero_vol_ratio': zero_vol_ratio.iloc[-1] if not zero_vol_ratio.empty else 0,\n",
    "            'amihud': amihud.iloc[-1] if not amihud.empty else 0,\n",
    "            'amihud_std': amihud_std.iloc[-1] if not amihud_std.empty else 0,\n",
    "            'volatility': volatility.iloc[-1] if not volatility.empty else 0\n",
    "        }\n",
    "        \n",
    "        # Apply normalization if parameters are available\n",
    "        if self.normalization_params['initialized']:\n",
    "            metrics['rolling_vol_norm'] = self.normalize_metric(metrics['rolling_vol'], 'volume')\n",
    "            metrics['amihud_norm'] = self.normalize_metric(metrics['amihud'], 'amihud')\n",
    "        else:\n",
    "            metrics['rolling_vol_norm'] = metrics['rolling_vol']\n",
    "            metrics['amihud_norm'] = metrics['amihud']\n",
    "            \n",
    "        return metrics\n",
    "\n",
    "    def update_ema_state(self, ticker, new_metrics, timestamp):\n",
    "        \"\"\"Update EMA-based liquidity state with normalized values\"\"\"\n",
    "        vol_metric = new_metrics.get('rolling_vol_norm', new_metrics['rolling_vol'])\n",
    "        amihud_metric = new_metrics.get('amihud_norm', new_metrics['amihud'])\n",
    "        \n",
    "        if ticker not in self.liquidity_states:\n",
    "            self.liquidity_states[ticker] = {\n",
    "                'fast_vol_ema': vol_metric,\n",
    "                'medium_vol_ema': vol_metric, \n",
    "                'slow_vol_ema': vol_metric,\n",
    "                'fast_amihud_ema': amihud_metric,\n",
    "                'medium_amihud_ema': amihud_metric,\n",
    "                'slow_amihud_ema': amihud_metric,\n",
    "                'last_update': timestamp,\n",
    "                'raw_vol': new_metrics['rolling_vol'],\n",
    "                'raw_amihud': new_metrics['amihud']\n",
    "            }\n",
    "        else:\n",
    "            state = self.liquidity_states[ticker]\n",
    "            \n",
    "            # Update volume EMAs with normalized values\n",
    "            state['fast_vol_ema'] = (self.ema_params['fast'] * vol_metric + \n",
    "                                    (1 - self.ema_params['fast']) * state['fast_vol_ema'])\n",
    "            state['medium_vol_ema'] = (self.ema_params['medium'] * vol_metric + \n",
    "                                     (1 - self.ema_params['medium']) * state['medium_vol_ema'])\n",
    "            state['slow_vol_ema'] = (self.ema_params['slow'] * vol_metric + \n",
    "                                   (1 - self.ema_params['slow']) * state['slow_vol_ema'])\n",
    "            \n",
    "            # Update Amihud EMAs with normalized values\n",
    "            state['fast_amihud_ema'] = (self.ema_params['fast'] * amihud_metric + \n",
    "                                      (1 - self.ema_params['fast']) * state['fast_amihud_ema'])\n",
    "            state['medium_amihud_ema'] = (self.ema_params['medium'] * amihud_metric + \n",
    "                                        (1 - self.ema_params['medium']) * state['medium_amihud_ema'])\n",
    "            state['slow_amihud_ema'] = (self.ema_params['slow'] * amihud_metric + \n",
    "                                      (1 - self.ema_params['slow']) * state['slow_amihud_ema'])\n",
    "            \n",
    "            state['last_update'] = timestamp\n",
    "            state['raw_vol'] = new_metrics['rolling_vol']\n",
    "            state['raw_amihud'] = new_metrics['amihud']\n",
    "\n",
    "    def prepare_ml_features(self, ticker, timestamp, current_metrics):\n",
    "        \"\"\"Prepare normalized features for ML prediction\"\"\"\n",
    "        time_features = self.extract_time_features(timestamp)\n",
    "        \n",
    "        features = {\n",
    "            **time_features,\n",
    "            'current_vol_norm': current_metrics.get('rolling_vol_norm', current_metrics['rolling_vol']),\n",
    "            'current_amihud_norm': current_metrics.get('amihud_norm', current_metrics['amihud']),\n",
    "            'vol_std_norm': current_metrics['vol_std'] / (current_metrics.get('vol_mad', 1) + 1e-8),\n",
    "            'volatility_norm': current_metrics['volatility'],\n",
    "            'zero_vol_ratio': current_metrics['zero_vol_ratio']\n",
    "        }\n",
    "        \n",
    "        if ticker in self.liquidity_states:\n",
    "            state = self.liquidity_states[ticker]\n",
    "            features.update({\n",
    "                'fast_vol_ema': state['fast_vol_ema'],\n",
    "                'medium_vol_ema': state['medium_vol_ema'],\n",
    "                'slow_vol_ema': state['slow_vol_ema'],\n",
    "                'fast_amihud_ema': state['fast_amihud_ema'],\n",
    "                'medium_amihud_ema': state['medium_amihud_ema'],\n",
    "                'slow_amihud_ema': state['slow_amihud_ema']\n",
    "            })\n",
    "        else:\n",
    "            features.update({\n",
    "                'fast_vol_ema': features['current_vol_norm'],\n",
    "                'medium_vol_ema': features['current_vol_norm'],\n",
    "                'slow_vol_ema': features['current_vol_norm'],\n",
    "                'fast_amihud_ema': features['current_amihud_norm'],\n",
    "                'medium_amihud_ema': features['current_amihud_norm'],\n",
    "                'slow_amihud_ema': features['current_amihud_norm']\n",
    "            })\n",
    "        \n",
    "        if ticker in self.liquidity_states:\n",
    "            state = self.liquidity_states[ticker]\n",
    "            vol_momentum = features['current_vol_norm'] - state['medium_vol_ema']\n",
    "            amihud_momentum = features['current_amihud_norm'] - state['medium_amihud_ema']\n",
    "            \n",
    "            features.update({\n",
    "                'vol_momentum': vol_momentum,\n",
    "                'amihud_momentum': amihud_momentum,\n",
    "                'vol_ema_spread': state['fast_vol_ema'] - state['slow_vol_ema'],\n",
    "                'amihud_ema_spread': state['fast_amihud_ema'] - state['slow_amihud_ema']\n",
    "            })\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def train_models(self, training_data):\n",
    "        \"\"\"Train ML models on historical data with proper normalization\"\"\"\n",
    "        if not training_data:\n",
    "            print(\"No training data available\")\n",
    "            return\n",
    "        \n",
    "        print(\"Updating normalization parameters...\")\n",
    "        self.update_normalization_params(training_data)\n",
    "        \n",
    "        X, y_vol, y_amihud = [], [], []\n",
    "        \n",
    "        for ticker, ticker_data in training_data.items():\n",
    "            if not isinstance(ticker_data, pd.DataFrame) or ticker_data.empty:\n",
    "                continue\n",
    "            metrics = self.compute_base_liquidity_metrics(ticker_data, ticker)\n",
    "            if metrics is None:\n",
    "                continue\n",
    "            for idx, row in ticker_data.iterrows():\n",
    "                timestamp = idx\n",
    "                features = self.prepare_ml_features(ticker, timestamp, metrics)\n",
    "                X.append(list(features.values()))\n",
    "                y_vol.append(self.normalize_metric(row.get('volume', 0), 'volume'))\n",
    "                y_amihud.append(self.normalize_metric(\n",
    "                    (row['close'].pct_change().abs() / row['volume'] if row['volume'] != 0 else 0), 'amihud'\n",
    "                ))\n",
    "        \n",
    "        if not X:\n",
    "            print(\"No valid training data after processing\")\n",
    "            return\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y_vol = np.array(y_vol)\n",
    "        y_amihud = np.array(y_amihud)\n",
    "        \n",
    "        X_scaled = self.feature_scaler.fit_transform(X)\n",
    "        X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        y_vol = np.nan_to_num(y_vol, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        y_amihud = np.nan_to_num(y_amihud, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        self.volume_predictor = GradientBoostingRegressor(\n",
    "            n_estimators=100, \n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        self.amihud_predictor = GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1, \n",
    "            max_depth=6,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        self.volume_predictor.fit(X_scaled, y_vol)\n",
    "        self.amihud_predictor.fit(X_scaled, y_amihud)\n",
    "        \n",
    "        vol_pred = self.volume_predictor.predict(X_scaled)\n",
    "        amihud_pred = self.amihud_predictor.predict(X_scaled)\n",
    "        \n",
    "        vol_rmse = np.sqrt(mean_squared_error(y_vol, vol_pred))\n",
    "        amihud_rmse = np.sqrt(mean_squared_error(y_amihud, amihud_pred))\n",
    "        \n",
    "        print(f\"Models trained successfully\")\n",
    "        print(f\"Volume RMSE: {vol_rmse:.4f}\")\n",
    "        print(f\"Amihud RMSE: {amihud_rmse:.4f}\")\n",
    "        \n",
    "        self.save_models()\n",
    "        self.save_normalization_params()\n",
    "\n",
    "    def predict_future_liquidity(self, ticker, timestamp, current_metrics, horizon_minutes=60):\n",
    "        \"\"\"Predict future liquidity using ML models\"\"\"\n",
    "        if self.volume_predictor is None or self.amihud_predictor is None:\n",
    "            return current_metrics['rolling_vol'], current_metrics['amihud']\n",
    "        \n",
    "        features = self.prepare_ml_features(ticker, timestamp, current_metrics)\n",
    "        feature_array = np.array([list(features.values())])\n",
    "        \n",
    "        try:\n",
    "            feature_array_scaled = self.feature_scaler.transform(feature_array)\n",
    "            predicted_vol = self.volume_predictor.predict(feature_array_scaled)[0]\n",
    "            predicted_amihud = self.amihud_predictor.predict(feature_array_scaled)[0]\n",
    "            \n",
    "            predicted_vol = self.denormalize_metric(predicted_vol, 'volume')\n",
    "            predicted_amihud = self.denormalize_metric(predicted_amihud, 'amihud')\n",
    "            \n",
    "            return max(0, predicted_vol), max(0, predicted_amihud)\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error for {ticker}: {e}\")\n",
    "            return current_metrics['rolling_vol'], current_metrics['amihud']\n",
    "\n",
    "    def compute_liquidity_score(self, vol_metric, amihud_metric, zero_vol_ratio, \n",
    "                              vol_threshold=1000, amihud_threshold=0.0001, zero_ratio_threshold=0.15):\n",
    "        \"\"\"Compute composite liquidity score (0-1, higher = more liquid)\"\"\"\n",
    "        vol_score = 1 / (1 + np.exp(-(vol_metric - vol_threshold) / (vol_threshold * 0.5)))\n",
    "        amihud_score = 1 / (1 + np.exp((amihud_metric - amihud_threshold) / (amihud_threshold * 2)))\n",
    "        zero_vol_score = 1 - min(1, zero_vol_ratio / zero_ratio_threshold)\n",
    "        \n",
    "        composite_score = 0.5 * vol_score + 0.3 * amihud_score + 0.2 * zero_vol_score\n",
    "        \n",
    "        return {\n",
    "            'composite_score': composite_score,\n",
    "            'vol_score': vol_score,\n",
    "            'amihud_score': amihud_score,\n",
    "            'zero_vol_score': zero_vol_score,\n",
    "            'is_liquid': composite_score > 0.5\n",
    "        }\n",
    "\n",
    "    def predict_adaptive_liquidity(self, trade_date, current_time, tickers=None, \n",
    "                                 vol_threshold=1000, zero_ratio_threshold=0.15, \n",
    "                                 amihud_threshold=0.0001):\n",
    "        \"\"\"Main method: Predict liquidity adaptively based on current time\"\"\"\n",
    "        trade_datetime = pd.Timestamp(f\"{trade_date} {current_time}\")\n",
    "        \n",
    "        if trade_datetime.hour == 9 and trade_datetime.minute <= 15:\n",
    "            end_data_time = trade_datetime.replace(hour=9, minute=15) - timedelta(days=1)\n",
    "            start_data_time = end_data_time - timedelta(days=self.lookback_days)\n",
    "            use_current_day = False\n",
    "        else:\n",
    "            start_data_time = trade_datetime.replace(hour=9, minute=15)\n",
    "            end_data_time = trade_datetime\n",
    "            use_current_day = True\n",
    "        \n",
    "        if tickers is None:\n",
    "            tickers = self.reader.get_available_tickers(trade_date)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                historical_df = self.reader.get_stock_data(\n",
    "                    [ticker],\n",
    "                    start=start_data_time - timedelta(days=self.lookback_days),\n",
    "                    end=start_data_time,\n",
    "                    columns=[\"date\", \"time\", \"close\", \"volume\", \"open\"],\n",
    "                    impute=False\n",
    "                ).get(ticker, pd.DataFrame())\n",
    "                \n",
    "                current_day_df = None\n",
    "                if use_current_day:\n",
    "                    current_day_df = self.reader.get_stock_data(\n",
    "                        [ticker],\n",
    "                        start=start_data_time,\n",
    "                        end=end_data_time,\n",
    "                        columns=[\"date\", \"time\", \"close\", \"volume\", \"open\"],\n",
    "                        impute=False\n",
    "                    ).get(ticker, pd.DataFrame())\n",
    "                \n",
    "                combined_df = historical_df\n",
    "                if current_day_df is not None and not current_day_df.empty:\n",
    "                    combined_df = pd.concat([historical_df, current_day_df]).sort_index()\n",
    "                \n",
    "                if combined_df.empty:\n",
    "                    print(f\"No data available for {ticker}\")\n",
    "                    continue\n",
    "                \n",
    "                current_metrics = self.compute_base_liquidity_metrics(combined_df, ticker)\n",
    "                if current_metrics is None:\n",
    "                    print(f\"Failed to compute metrics for {ticker}\")\n",
    "                    continue\n",
    "                \n",
    "                self.update_ema_state(ticker, current_metrics, trade_datetime)\n",
    "                \n",
    "                predicted_vol, predicted_amihud = self.predict_future_liquidity(\n",
    "                    ticker, trade_datetime, current_metrics\n",
    "                )\n",
    "                \n",
    "                current_score = self.compute_liquidity_score(\n",
    "                    current_metrics['rolling_vol'], \n",
    "                    current_metrics['amihud'],\n",
    "                    current_metrics['zero_vol_ratio'],\n",
    "                    vol_threshold, amihud_threshold, zero_ratio_threshold\n",
    "                )\n",
    "                \n",
    "                predicted_score = self.compute_liquidity_score(\n",
    "                    predicted_vol, predicted_amihud,\n",
    "                    current_metrics['zero_vol_ratio'],\n",
    "                    vol_threshold, amihud_threshold, zero_ratio_threshold\n",
    "                )\n",
    "                \n",
    "                results[ticker] = {\n",
    "                    'timestamp': trade_datetime,\n",
    "                    'current_liquidity': current_score,\n",
    "                    'predicted_liquidity': predicted_score,\n",
    "                    'current_metrics': current_metrics,\n",
    "                    'predicted_metrics': {\n",
    "                        'volume': predicted_vol,\n",
    "                        'amihud': predicted_amihud\n",
    "                    },\n",
    "                    'ema_state': self.liquidity_states[ticker].copy() if ticker in self.liquidity_states else None\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {ticker}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def save_models(self):\n",
    "        \"\"\"Save trained models and scaler\"\"\"\n",
    "        model_data = {\n",
    "            'volume_predictor': self.volume_predictor,\n",
    "            'amihud_predictor': self.amihud_predictor,\n",
    "            'feature_scaler': self.feature_scaler,\n",
    "            'ema_params': self.ema_params\n",
    "        }\n",
    "        \n",
    "        with open(self.model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "    def load_models(self):\n",
    "        \"\"\"Load saved models\"\"\"\n",
    "        if self.model_path.exists():\n",
    "            try:\n",
    "                with open(self.model_path, 'rb') as f:\n",
    "                    model_data = pickle.load(f)\n",
    "                \n",
    "                self.volume_predictor = model_data.get('volume_predictor')\n",
    "                self.amihud_predictor = model_data.get('amihud_predictor')\n",
    "                self.feature_scaler = model_data.get('feature_scaler', RobustScaler())\n",
    "                self.ema_params = model_data.get('ema_params', self.ema_params)\n",
    "                \n",
    "                print(\"Models loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading models: {e}\")\n",
    "\n",
    "    def save_normalization_params(self):\n",
    "        \"\"\"Save normalization parameters\"\"\"\n",
    "        with open(self.normalization_path, 'wb') as f:\n",
    "            pickle.dump(self.normalization_params, f)\n",
    "\n",
    "    def load_normalization_params(self):\n",
    "        \"\"\"Load normalization parameters\"\"\"\n",
    "        if self.normalization_path.exists():\n",
    "            try:\n",
    "                with open(self.normalization_path, 'rb') as f:\n",
    "                    self.normalization_params = pickle.load(f)\n",
    "                print(\"Normalization parameters loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading normalization parameters: {e}\")\n",
    "\n",
    "# reader = StockDataReader()\n",
    "liquid_tickers = liquid_data.keys()\n",
    "predictor = AdaptiveLiquidityPredictor(reader)\n",
    "\n",
    "results_open = predictor.predict_adaptive_liquidity(\n",
    "    trade_date=\"2022-02-01\",\n",
    "    current_time=\"09:15\",\n",
    "    tickers=liquid_tickers\n",
    ")\n",
    "\n",
    "results_mid = predictor.predict_adaptive_liquidity(\n",
    "    trade_date=\"2022-02-01\", \n",
    "    current_time=\"14:30\",\n",
    "    tickers=liquid_tickers\n",
    ")\n",
    "\n",
    "for ticker, result in results_open.items():\n",
    "    current_liq = result['current_liquidity']\n",
    "    predicted_liq = result['predicted_liquidity']\n",
    "    \n",
    "    print(f\"\\n{ticker} at 09:15:\")\n",
    "    print(f\"Current Liquidity Score: {current_liq['composite_score']:.3f} ({'Liquid' if current_liq['is_liquid'] else 'Illiquid'})\")\n",
    "    print(f\"Predicted Liquidity Score: {predicted_liq['composite_score']:.3f} ({'Liquid' if predicted_liq['is_liquid'] else 'Illiquid'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1afe6d-7c30-44ee-b8bf-f8cf758d0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ticker, result in results_mid.items():\n",
    "    current_liq = result['current_liquidity']\n",
    "    predicted_liq = result['predicted_liquidity'] \n",
    "    \n",
    "    print(f\"\\n{ticker} at 09:15:\")\n",
    "    print(f\"Current Liquidity Score: {current_liq['composite_score']:.3f} ({'Liquid' if current_liq['is_liquid'] else 'Illiquid'})\")\n",
    "    print(f\"Predicted Liquidity Score: {predicted_liq['composite_score']:.3f} ({'Liquid' if predicted_liq['is_liquid'] else 'Illiquid'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd75cf8-c3bc-45fe-ab0f-efce704b45a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed147b9a-f08a-479b-90d9-16e55fa55901",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_volume_tickers = {ticker  for ticker, df in dfs.items() if df.volume.sum() == 0}\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651ccab-6f40-4c9e-b45e-1ec3804191e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {ticker: df for ticker, df in dfs.items() if ticker not in ticker zero_volume_tickers}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10c52a-e0d3-4b78-a530-02ff90de1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = get_stock_data(\n",
    "#     tickers,\n",
    "#     impute=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4279aa-081a-4465-ab2c-44a56a148916",
   "metadata": {},
   "source": [
    "Mean trading volume for all the stocks for the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b83a6-61e2-4d0b-909a-5aa7005a1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mean_monthly_traded_volume = sum(df.volume.mean() for _, df in dfs.items())/len(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d02de5-c405-4a5f-9080-340e2e9555e8",
   "metadata": {},
   "source": [
    " Here we find the stocks whichh have high trading mean trading volumes for atleast half the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c0ca3-f258-4766-bea0-afb97c7c1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate high volume day count for each stock\n",
    "high_volume_day_counts = {\n",
    "    s: (df.groupby(df.index.date)['volume'].mean() > overall_mean_monthly_traded_volume).sum().item()\n",
    "    for s, df in dfs.items()\n",
    "}\n",
    "\n",
    "# Histogram \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(high_volume_day_counts.values(), bins=10, edgecolor='black')\n",
    "plt.xlabel('Number of High-Volume Days')\n",
    "plt.ylabel('Number of Stocks')\n",
    "plt.title('Distribution of High-Volume Days Across Stocks')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d6d95-055b-4100-8fde-09aa81820118",
   "metadata": {},
   "source": [
    "We observe that for more than half of January, there was sufficient liquidity in the market for some of the stocks. A huge number of stocks have been illiquid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70cb61-63de-422e-adcc-2ff1891b5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "liquid_stocks = [ s for s, v in high_volume_day_counts.items() if v > 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880ff59-6ce1-42b3-8352-3b326a4d5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "def calculate_half_life(price_series):\n",
    "    \"\"\"\n",
    "    Calculates the half-life of mean reversion for a price series using linear regression.\n",
    "    \n",
    "    Reference:\n",
    "        Chan, E. P. (2013). Algorithmic Trading: Winning Strategies and Their Rationale. \n",
    "        John Wiley & Sons.\n",
    "    \"\"\"\n",
    "    delta_p = price_series.diff().dropna()\n",
    "    p_lag = price_series.shift(1).dropna()\n",
    "    \n",
    "    # Align indices explicitly - keep only indices that appear in both\n",
    "    common_idx = delta_p.index.intersection(p_lag.index)\n",
    "    delta_p = delta_p.loc[common_idx]\n",
    "    p_lag = p_lag.loc[common_idx]\n",
    "    \n",
    "    model = OLS(delta_p.values, add_constant(p_lag.values))\n",
    "    res = model.fit()\n",
    "    beta = res.params[1]\n",
    "    halflife = -np.log(2) / beta if beta < 0 else np.inf\n",
    "    return halflife\n",
    "\n",
    "def hurst_exponent(ts):\n",
    "    lags = range(2, 100)\n",
    "    tau = [np.sqrt(np.std(np.subtract(ts[lag:], ts[:-lag]))) for lag in lags]\n",
    "    poly = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "    return poly[0] * 2.0\n",
    "\n",
    "def mean_reversion_vector(price_series):\n",
    "    price_series = price_series.dropna()\n",
    "    \n",
    "    adf_stat = adfuller(price_series)[0]\n",
    "    half_life = calculate_half_life(price_series)\n",
    "    hurst = hurst_exponent(price_series.values)\n",
    "    \n",
    "    returns = price_series.pct_change().dropna()\n",
    "    vol = returns.std()\n",
    "    \n",
    "    return np.array([adf_stat, half_life, hurst, vol])\n",
    "\n",
    "# Now your embedding extraction code can stay the same:\n",
    "embedding_list = []\n",
    "valid_stocks = []\n",
    "\n",
    "# for ticker in liquid_stocks:\n",
    "#     df = dfs[ticker]\n",
    "#     price_series = df['close']\n",
    "    \n",
    "#     try:\n",
    "#         vec = mean_reversion_vector(price_series)\n",
    "#         if np.all(np.isfinite(vec)):\n",
    "#             embedding_list.append(vec)\n",
    "#             valid_stocks.append(ticker)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed for {ticker}: {e}\")\n",
    "\n",
    "# embeddings = np.vstack(embedding_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155e8cf-75e4-44c2-93d5-0fecca037bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def process_ticker(ticker, price_series):\n",
    "    \"\"\"Compute mean-reversion vector for one ticker\"\"\"\n",
    "    try:\n",
    "        vec = mean_reversion_vector(price_series)\n",
    "        if np.all(np.isfinite(vec)):\n",
    "            return ticker, vec\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {ticker}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Prepare inputs first (avoid passing big dfs dict to workers)\n",
    "inputs = [(ticker, dfs[ticker]['close']) for ticker in liquid_stocks]\n",
    "\n",
    "results = Parallel(n_jobs=8, backend=\"loky\")(  # try 8 workers first\n",
    "    delayed(process_ticker)(ticker, price_series) \n",
    "    for ticker, price_series in inputs\n",
    ")\n",
    "\n",
    "valid_results = [r for r in results if r is not None]\n",
    "valid_stocks, embedding_list = zip(*valid_results)\n",
    "embeddings = np.vstack(embedding_list)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece7f3f-9ad8-455d-89f4-0dd61f76412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Cosine similarity matrix\n",
    "cos_sim = cosine_similarity(embeddings)\n",
    "\n",
    "# Cluster \n",
    "cluster_model = AgglomerativeClustering(n_clusters=20, linkage='average')\n",
    "\n",
    "distances = 1 - cos_sim\n",
    "\n",
    "labels = cluster_model.fit_predict(distances)\n",
    "\n",
    "# Now labels[i] gives cluster of valid_stocks[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07131d4-5814-4d0f-88cc-8c597b102a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbb95ba-158d-4dd2-9ac4-6ac777a479eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import umap.umap_ as umap \n",
    "\n",
    "reducer = umap.plot.interactive(min_dist =0.002, n_components=6, n_neighbors=30)\n",
    "embeddings_umap = reducer.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(embeddings_umap[:, 0], embeddings_umap[:, 1], c=labels, cmap='tab10', s=50, alpha=0.8)\n",
    "plt.title(\"Stocks clustered by mean-reversion indicators (UMAP 2D projection)\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.colorbar(scatter, label='Cluster label')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932f415-ff21-4223-ab5f-fdafcb061c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "clustered_tickers = defaultdict(list)\n",
    "for ticker, label in zip(tickers, labels):\n",
    "    clustered_tickers[label].append(ticker)\n",
    "\n",
    "for label in sorted(clustered_tickers):\n",
    "    print(f\"\\nCluster {label} ({len(clustered_tickers[label])} tickers):\")\n",
    "    print(\", \".join(clustered_tickers[label]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde930b8-a73f-496c-a822-849292629433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_pair(label, ticker1, ticker2):\n",
    "    try:\n",
    "        s1 = dfs[ticker1]['close']\n",
    "        s2 = dfs[ticker2]['close']\n",
    "        joined = pd.concat([s1, s2], axis=1, join='inner').dropna()\n",
    "        if len(joined) < 50:\n",
    "            return None\n",
    "        s1_aligned = joined.iloc[:, 0]\n",
    "        s2_aligned = joined.iloc[:, 1]\n",
    "        score, pvalue, _ = coint(s1_aligned, s2_aligned)\n",
    "        return {\n",
    "            'cluster': label,\n",
    "            'pair': (ticker1, ticker2),\n",
    "            'cointegration_pvalue': pvalue\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "all_pairs = []\n",
    "for label, ticker_list in clustered_tickers.items():\n",
    "    if 2 <= len(ticker_list) <= 10:\n",
    "        for ticker1, ticker2 in combinations(ticker_list, 2):\n",
    "            all_pairs.append((label, ticker1, ticker2))\n",
    "pair_results = []\n",
    "for label, ticker1, ticker2 in all_pairs:\n",
    "    result = evaluate_pair(label, ticker1, ticker2)\n",
    "    if result is not None:\n",
    "        pair_results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780da69-7a41-42d1-b35f-55bb53fd6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "for result in pair_results:\n",
    "    t1, t2 = result['pair']\n",
    "    s1 = dfs[t1]['close']\n",
    "    s2 = dfs[t2]['close']\n",
    "    joined = pd.concat([s1, s2], axis=1, join='inner').dropna()\n",
    "    y = joined.iloc[:,0].values\n",
    "    x = joined.iloc[:,1].values\n",
    "    beta = OLS(y, add_constant(x)).fit().params[1]\n",
    "    spread = y - beta * x\n",
    "    adf_stat, adf_pvalue, *_ = adfuller(spread)\n",
    "    result['spread_adf_stat'] = adf_stat\n",
    "    result['spread_adf_pvalue'] = adf_pvalue\n",
    "    result['spread_std'] = np.std(spread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56db14bc-adbf-44e7-9ec8-ed51ef6742e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_pairs = [\n",
    "    r for r in pair_results\n",
    "    if r['cointegration_pvalue'] < 0.05 and r['spread_adf_pvalue'] < 0.05\n",
    "]\n",
    "#  sort by (1) p-value, (2) spread_std, (3) cluster size\n",
    "good_pairs = sorted(good_pairs, key=lambda r: (r['cointegration_pvalue'], -r['spread_std']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec84d08-98eb-4a2c-8dac-727375d7bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_report = pd.DataFrame(good_pairs)\n",
    "print(df_report[['cluster', 'pair', 'cointegration_pvalue', 'spread_adf_pvalue', 'spread_std']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fcc9a-7de1-42c9-9a35-78977ac9e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = good_pairs[0]['pair']\n",
    "s1 = dfs[pair[0]]['close']\n",
    "s2 = dfs[pair[1]]['close']\n",
    "joined = pd.concat([s1, s2], axis=1, join='inner').dropna()\n",
    "y = joined.iloc[:,0].values\n",
    "x = joined.iloc[:,1].values\n",
    "beta = OLS(y, add_constant(x)).fit().params[1]\n",
    "spread = y - beta * x\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(joined.index, spread)\n",
    "plt.title(f\"Spread between {pair[0]} and {pair[1]}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Spread\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b48c09-63b1-4576-b78f-f77615fba892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09019601-6f20-4ae5-a9e1-d7e4951c8b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
