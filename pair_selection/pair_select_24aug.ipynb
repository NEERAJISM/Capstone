{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc87b17-1206-4ccc-a7c4-bb007d217974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in c:\\users\\mangl\\desktop\\capstone\\venv\\lib\\site-packages (0.61.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\mangl\\desktop\\capstone\\venv\\lib\\site-packages (from numba) (0.44.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.24 in c:\\users\\mangl\\desktop\\capstone\\venv\\lib\\site-packages (from numba) (2.2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numba\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import calendar\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Optional, Callable, Dict, Union, Tuple\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "from joblib import Parallel, delayed\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.core.debugger import set_trace\n",
    "from itertools import chain, combinations, islice\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant\n",
    "from numba import njit\n",
    "# This code was developed with the assistance of multiple AI agents (e.g., Perplexity, August 2025).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3586e10-357e-4440-902c-423d615ee794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310d41cc-6991-45c0-a880-2d038550b758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\mangl\\\\Desktop\\\\capstone\\\\pair_selection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c0ee91-b2d2-437a-b36a-6d5b038ac38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StockDataLoader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tickers: List[str],\n",
    "        start: Union[str, datetime],\n",
    "        end: Union[str, datetime],\n",
    "        agg_func: Optional[Union[str, Callable]] = None,\n",
    "        resample_freq: Optional[str] = None,\n",
    "        select_columns: Optional[List[str]] = None,\n",
    "        impute: bool = True,\n",
    "        base_dir: Optional[Union[str, Path]] = \"../downloaded_files\",\n",
    "    ):\n",
    "        # Make attributes private\n",
    "        self._start = self._parse_datetime(start)\n",
    "        self._end = self._parse_datetime(end)\n",
    "        self._base_dir = Path(base_dir)\n",
    "        \n",
    "        self._tickers = tickers or self._find_available_tickers()\n",
    "        \n",
    "        self._agg_func = agg_func\n",
    "        self._resample_freq = resample_freq\n",
    "        self._select_columns = select_columns  \n",
    "        self._impute = impute\n",
    "\n",
    "        # Cache for storing loaded data\n",
    "        self._data_cache: Dict[str, pl.DataFrame] = {}\n",
    "\n",
    "        # Column mapping for renaming\n",
    "        self._rev_cols_map = {\n",
    "            \"open_interest\": \"<o/i> \",\n",
    "            \"date\": \"<date>\",\n",
    "            \"time\": \"<time>\",\n",
    "            \"open\": \"<open>\",\n",
    "            \"high\": \"<high>\",\n",
    "            \"low\": \"<low>\",\n",
    "            \"close\": \"<close>\",\n",
    "            \"volume\": \"<volume>\",\n",
    "        }\n",
    "        self._cols_map = {\n",
    "            \"<o/i> \": \"open_interest\",\n",
    "            \"<date>\": \"date\",\n",
    "            \"<time>\": \"time\",\n",
    "            \"<open>\": \"open\",\n",
    "            \"<high>\": \"high\",\n",
    "            \"<low>\": \"low\",\n",
    "            \"<close>\": \"close\",\n",
    "            \"<volume>\": \"volume\"\n",
    "        }\n",
    "        self._new_columns, self._columns = zip(*self._rev_cols_map.items())\n",
    "        if self._select_columns:\n",
    "            self._new_columns = self._select_columns + [\"date\", \"time\"]\n",
    "            self._columns = [self._rev_cols_map[c] for c in self._new_columns]\n",
    "\n",
    "        self._schema = {\n",
    "             \"date\": pl.String,\n",
    "            \"time\":pl.String,\n",
    "            \"datetime\": pl.Datetime(\"us\"),\n",
    "            \"open_price\": pl.Float64,\n",
    "            \"high_price\": pl.Float64,\n",
    "            \"low_price\": pl.Float64,\n",
    "            \"close_price\": pl.Float64,\n",
    "            \"volume\": pl.Float64,\n",
    "            \"open_interest\": pl.Float64,\n",
    "        }\n",
    "    def _find_available_tickers(self) -> List[str]:\n",
    "        \"\"\"Find all tickers present across all months by set intersection.\"\"\"\n",
    "        months = self._generate_monthly_files()\n",
    "        all_month_tickers = []\n",
    "\n",
    "        for month in months:\n",
    "            folder = self._get_month_dir(month)\n",
    "            if not folder.exists():\n",
    "                continue\n",
    "\n",
    "            tickers_in_month = {\n",
    "                f.stem for f in folder.glob(\"*.csv\") if f.is_file()\n",
    "            }\n",
    "            if tickers_in_month:\n",
    "                all_month_tickers.append(tickers_in_month)\n",
    "\n",
    "        # If no CSVs found at all\n",
    "        if not all_month_tickers:\n",
    "            raise FileNotFoundError(\n",
    "                f\"No CSV files found in any month between {self._start} and {self._end}\"\n",
    "            )\n",
    "\n",
    "        # Take intersection to get tickers present in *all* months\n",
    "        common_tickers = set.intersection(*all_month_tickers)\n",
    "        if not common_tickers:\n",
    "            raise ValueError(\n",
    "                \"No common tickers found across all months in the given date range.\"\n",
    "            )\n",
    "\n",
    "        return sorted(common_tickers)\n",
    "    def _parse_datetime(self, dt):\n",
    "        \"\"\"Ensure datetime parsing.\"\"\"\n",
    "        if isinstance(dt, datetime):\n",
    "            return dt\n",
    "        return datetime.strptime(dt, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def _generate_monthly_files(self):\n",
    "        \"\"\"Generate list of months between start and end.\"\"\"\n",
    "        start_month = self._start.replace(day=1)\n",
    "        end_month = self._end.replace(day=1)\n",
    "\n",
    "        months = []\n",
    "        while start_month <= end_month:\n",
    "            months.append(start_month.strftime(\"%Y-%m\"))  # e.g. \"2025-01\"\n",
    "            start_month += timedelta(days=32)\n",
    "            start_month = start_month.replace(day=1)\n",
    "        return months\n",
    "\n",
    "    def _get_month_dir(self, month: str) -> Path:\n",
    "        \"\"\"\n",
    "        Construct the directory path dynamically based on new structure:\n",
    "        ../downloaded_files/{year}/Cash Data {month_name} {year}\n",
    "        \"\"\"\n",
    "        \n",
    "        year, month_num = month.split(\"-\")\n",
    "        month_name = calendar.month_name[int(month_num)]\n",
    "        \n",
    "        return self._base_dir / year / f\"Cash Data {month_name} {year}\"\n",
    "\n",
    "    def _empty_df(self) -> pl.DataFrame:\n",
    "        \"\"\"Return an empty DataFrame with correct schema.\"\"\"\n",
    "        return pl.DataFrame(schema=self._schema)\n",
    "        \n",
    "    def _load_single_file(self, ticker: str, month: str) -> pl.DataFrame:\n",
    "        \"\"\"Load and preprocess a single month's CSV for one ticker.\"\"\"\n",
    "        folder = self._get_month_dir(month)\n",
    "        file_path = folder / f\"{ticker}.csv\"\n",
    "    \n",
    "        # If file doesn't exist â†’ return empty DataFrame with correct schema\n",
    "        if not file_path.exists():\n",
    "            return self._empty_df()\n",
    "    \n",
    "        # Read CSV with only required columns and rename them\n",
    "        df = pl.read_csv(file_path, columns=self._columns)\n",
    "        df = df.rename({i: self._cols_map[i] for i in df.columns})\n",
    "    \n",
    "        # Combine date & time into datetime column, sort, and drop originals\n",
    "        df = df.with_columns(\n",
    "            pl.concat_str([df[\"date\"], df[\"time\"]], separator=\" \")\n",
    "            .str.strptime(pl.Datetime, format=\"%m/%d/%Y %H:%M:%S\")\n",
    "            .alias(\"datetime\")\n",
    "        ).drop([\"date\", \"time\"]).sort(\"datetime\")\n",
    "    \n",
    "        df = df.set_sorted(\"datetime\")\n",
    "        return df\n",
    "    \n",
    "    def _merge_monthly_data(self, ticker: str) -> pl.DataFrame:\n",
    "        \"\"\"Read multiple monthly files, combine them, handle imputation, and resampling.\"\"\"\n",
    "        months = self._generate_monthly_files()\n",
    "        dfs = [self._load_single_file(ticker, month) for month in months]\n",
    "        dfs = [d for d in dfs if not d.is_empty()]\n",
    "    \n",
    "        if not dfs:\n",
    "            return self._empty_df()\n",
    "    \n",
    "        # 1) Concatenate all months\n",
    "        df = pl.concat(dfs, how=\"vertical\")\n",
    "    \n",
    "        # 2) Sort by datetime\n",
    "        df = df.sort(\"datetime\")\n",
    "    \n",
    "        # 3) Drop duplicates\n",
    "        df = df.unique(subset=[\"datetime\"], keep=\"last\", maintain_order=True)\n",
    "    \n",
    "        # 4) Trim to requested global range BEFORE imputation\n",
    "        df = df.filter((pl.col(\"datetime\") >= self._start) & (pl.col(\"datetime\") <= self._end))\n",
    "    \n",
    "        # 5) Global imputation: forward fill + backward fill\n",
    "        if self._impute:\n",
    "            time_range = pl.DataFrame({\n",
    "                \"datetime\": pl.datetime_range(\n",
    "                    start=self._start,\n",
    "                    end=self._end,\n",
    "                    interval=\"1m\",\n",
    "                    eager=True\n",
    "                )\n",
    "            })\n",
    "            df = time_range.join(df, on=\"datetime\", how=\"left\")\n",
    "            df = df.fill_null(strategy=\"forward\").fill_null(strategy=\"backward\")\n",
    "    \n",
    "        # 6) Resample if needed\n",
    "        if not df.is_empty() and self._resample_freq:\n",
    "            func = self._agg_func or \"mean\"\n",
    "            df = df.group_by_dynamic(\n",
    "                time_column=\"datetime\",\n",
    "                every=self._resample_freq,\n",
    "                closed=\"left\"\n",
    "            ).agg({col: func for col in df.columns if col != \"datetime\"})\n",
    "            df = df.sort(\"datetime\")\n",
    "    \n",
    "        # 7) Keep only selected columns\n",
    "        keep_cols = [\"datetime\"] + [col for col in self._new_columns if col in df.columns]\n",
    "        return df.select(keep_cols)\n",
    "\n",
    "    def get_data_for_tickers(self, tickers: Optional[List[str]] = None) -> Dict[str, pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Public method: Returns processed data for one or more tickers.\n",
    "        Uses cache when available.\n",
    "        Fully parallelized using joblib with all available CPU cores.\n",
    "        \"\"\"\n",
    "        if not tickers:\n",
    "            tickers = self._tickers\n",
    "        else:\n",
    "            tickers = [t for t in tickers if t in self._tickers]\n",
    "\n",
    "        if not tickers:\n",
    "            raise ValueError(\"No valid tickers provided or found.\")\n",
    "\n",
    "        # Use all available cores\n",
    "        n_jobs = os.cpu_count() or 1\n",
    "\n",
    "        # Run in parallel for all tickers\n",
    "        results_list = Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
    "            delayed(self._load_or_get_from_cache)(ticker) for ticker in tickers\n",
    "        )\n",
    "\n",
    "        # Combine ticker names with their data\n",
    "        return dict(zip(tickers, results_list))\n",
    "\n",
    "    def _load_or_get_from_cache(self, ticker: str) -> pl.DataFrame:\n",
    "        \"\"\"Helper to either fetch from cache or load fresh.\"\"\"\n",
    "        if ticker not in self._data_cache:\n",
    "            self._data_cache[ticker] = self._merge_monthly_data(ticker)\n",
    "        return self._data_cache[ticker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e12283-2c3b-4af8-b6b5-84b83f736bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = StockDataLoader(\n",
    "#     tickers=[],\n",
    "#     start=\"2021-12-02 09:15:00\",\n",
    "#     end=\"2022-03-02 15:30:00\",\n",
    "#     # resample_freq=\"5m\",\n",
    "#     select_columns=[\"volume\"],\n",
    "#     base_dir=\"../downloaded_files\",\n",
    "#     impute = True\n",
    "# )\n",
    "# data_dict = loader.get_data_for_tickers();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d064a5-f282-4ca5-a33a-17463074375b",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class LiquidityPredictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_dir: str,\n",
    "        run_date: str,\n",
    "        lookback: int,\n",
    "        trading_start: str = \"09:15:00\",\n",
    "        trading_end: str = \"15:30:00\",\n",
    "        time_interval_minutes: int = 60,\n",
    "        tickers: Optional[List[str]] = None\n",
    "    ):\n",
    "        self.base_dir = base_dir\n",
    "        \n",
    "        # Convert run_date to datetime\n",
    "        self.run_date = datetime.strptime(run_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        # Set start and end dates implicitly\n",
    "        self.end_day = self.run_date\n",
    "        self.start_day = self.run_date - timedelta(days=lookback)\n",
    "        \n",
    "        # Market timings\n",
    "        self.start_time = datetime.strptime(trading_start, \"%H:%M:%S\").time()\n",
    "        self.end_time = datetime.strptime(trading_end, \"%H:%M:%S\").time()\n",
    "        self.time_interval = timedelta(minutes=time_interval_minutes)\n",
    "        \n",
    "        # Stock tickers (optional)\n",
    "        self.tickers = tickers\n",
    "        \n",
    "        # Logging for debugging\n",
    "        self._log_initial_config()\n",
    "\n",
    "    def _log_initial_config(self):\n",
    "        print(f\"Base directory: {self.base_dir}\")\n",
    "        print(f\"Run Date: {self.run_date.date()}\")\n",
    "        print(f\"Start Day: {self.start_day.date()}\")\n",
    "        print(f\"End Day: {self.end_day.date()}\")\n",
    "        print(f\"Trading Window: {self.start_time} â†’ {self.end_time}\")\n",
    "        print(f\"Tickers: {self.tickers if self.tickers else 'Using default universe'}\")\n",
    "\n",
    "        \n",
    "    def filter_basic(self, volume_threshold: int = 1000) -> List[str]:\n",
    "        \"\"\"\n",
    "        Basic filtering by average volume.\n",
    "        Returns tickers passing the volume threshold.\n",
    "        \"\"\"\n",
    "        loader = StockDataLoader(\n",
    "            tickers=self.tickers,\n",
    "            start=f\"{self.start_day.strftime('%Y-%m-%d')} {self.start_time}\",\n",
    "            end=f\"{self.end_day.strftime('%Y-%m-%d')} {self.end_time}\",\n",
    "            select_columns=[\"volume\"],\n",
    "            base_dir=self.base_dir\n",
    "        )\n",
    "        data_dict = loader.get_data_for_tickers()\n",
    "        filtered = []\n",
    "\n",
    "        for ticker, df in data_dict.items():\n",
    "            if not df.is_empty() and df[\"volume\"].mean() >= volume_threshold:\n",
    "                filtered.append(ticker)\n",
    "\n",
    "        return filtered\n",
    "\n",
    "    def filter_advanced(\n",
    "        self,\n",
    "        tickers: List[str],\n",
    "        custom_weights: Optional[Dict[str, float]] = None,\n",
    "        min_score: float = 30.0\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Advanced filtering: compute metrics, weighted score, and return ranked tickers.\n",
    "        Returns list of tickers sorted by liquidity (highest first).\n",
    "        \"\"\"\n",
    "        if custom_weights is None:\n",
    "            custom_weights = {\n",
    "                \"average_volume\": 0.4,\n",
    "                \"trading_frequency\": 0.3,\n",
    "                \"amihud_liquidity\": 0.2,\n",
    "                \"volume_consistency\": 0.1\n",
    "            }\n",
    "\n",
    "        loader = StockDataLoader(\n",
    "            tickers=tickers,\n",
    "            start=f\"{self.start_day.strftime('%Y-%m-%d')} {self.start_time}\",\n",
    "            end=f\"{self.end_day.strftime('%Y-%m-%d')} {self.end_time}\",\n",
    "            select_columns=[\"volume\", \"close\", \"open\"],\n",
    "            base_dir=self.base_dir\n",
    "        )\n",
    "        data_dict = loader.get_data_for_tickers()\n",
    "\n",
    "        scores = []\n",
    "        for ticker, df in data_dict.items():\n",
    "            score = self._compute_weighted_score(df, custom_weights)\n",
    "            if score is not None and score >= min_score:\n",
    "                scores.append((ticker, score))\n",
    "\n",
    "        # Sort by descending score\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [t[0] for t in scores]\n",
    "\n",
    "    def _compute_weighted_score(self, df: pl.DataFrame, weights: Dict[str, float]) -> Optional[float]:\n",
    " \n",
    "        avg_volume = self._compute_average_volume_score(df)\n",
    "        trading_freq = self._compute_trading_frequency_score(df)\n",
    "        amihud = self._compute_amihud_liquidity_score(df)\n",
    "        volume_consistency = self._compute_volume_consistency_score(df)\n",
    "     \n",
    "        return (\n",
    "            avg_volume * weights[\"average_volume\"] +\n",
    "            trading_freq * weights[\"trading_frequency\"] +\n",
    "            amihud * weights[\"amihud_liquidity\"] +\n",
    "            volume_consistency * weights[\"volume_consistency\"]\n",
    "        )\n",
    "\n",
    "    def _compute_average_volume_score(self, df: pl.DataFrame) -> float:\n",
    "        \"\"\"Log-normalized average volume (0-100).\"\"\"\n",
    "        mean = df[\"volume\"].mean()\n",
    "        return min(100, max(0, (np.log10(mean) - 2) / 5 * 100))\n",
    "    \n",
    "    def _compute_trading_frequency_score(self, df: pl.DataFrame) -> float:\n",
    "        \"\"\"Percentage of periods with volume > 0.\"\"\"\n",
    "        total = df.height\n",
    "        active = df.filter(pl.col(\"volume\") > 0).height\n",
    "        return active / total * 100\n",
    "        \n",
    "    def _compute_amihud_liquidity_score(self, df: pl.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        Computes the Amihud Illiquidity Score.\n",
    "        Lower score = more liquid, higher = less liquid.\n",
    "        Formula: ILLIQ = (1/D) * Î£(|R| / VOL)\n",
    "        \"\"\"\n",
    "        df = df.with_columns((pl.col(\"close\").pct_change().abs()).alias(\"return\"))    \n",
    "        df = df.filter(pl.col(\"volume\") > 0)\n",
    "    \n",
    "        df = df.with_columns((pl.col(\"return\") / pl.col(\"volume\")).alias(\"amihud\"))\n",
    "        illiq = df[\"amihud\"].mean()\n",
    "        return illiq\n",
    "\n",
    "    def _compute_volume_consistency_score(self, df: pl.DataFrame) -> float:\n",
    "        \"\"\"Score based on volume coefficient of variation (CV).\"\"\"\n",
    "        mean = df[\"volume\"].mean()\n",
    "        std = df[\"volume\"].std()\n",
    "        cv = std / mean\n",
    "        return 100 - cv * 30 if cv <= 1 else 70 * np.exp(-(cv - 1) * 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d05106a-ccf9-42d2-bee4-d710d82a3a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: ../downloaded_files\n",
      "Run Date: 2021-04-01\n",
      "Start Day: 2021-01-31\n",
      "End Day: 2021-04-01\n",
      "Trading Window: 09:15:00 â†’ 15:30:00\n",
      "Tickers: Using default universe\n",
      "Tickers passing basic volume filter: ['3IINFOTECH', 'A2ZINFRA', 'ABCAPITAL', 'ABFRL', 'ACE', 'ADANIENT', 'ADANIGREEN', 'ADANIPORTS', 'ADANIPOWER', 'ADANITRANS', 'AGROPHOS', 'AIRAN', 'AKSHOPTFBR', 'ALCHEM', 'ALEMBICLTD', 'ALOKINDS', 'ALPSINDUS', 'AMBUJACEM', 'ANANTRAJ', 'ANDHRACEMT', 'ANKITMETAL', 'ANSALAPI', 'APOLLOHOSP', 'APOLLOTYRE', 'ARCHIES', 'ARVIND', 'ASHIMASYN', 'ASHOKA', 'ASHOKLEY', 'ASIANPAINT', 'ATGL', 'AUBANK', 'AUROPHARMA', 'AVTNPL', 'AXISBANK', 'BAGFILMS', 'BAJAJCON', 'BAJAJHIND', 'BAJFINANCE', 'BALLARPUR', 'BALMLAWRIE', 'BALRAMCHIN', 'BANDHANBNK', 'BANKBARODA', 'BANKBEES', 'BANKINDIA', 'BCG', 'BCP', 'BEL', 'BEPL', 'BHANDARI', 'BHARATFORG', 'BHARTIARTL', 'BHEL', 'BILENERGY', 'BIOCON', 'BKMINDST', 'BLKASHYAP', 'BLUECHIP', 'BODALCHEM', 'BOMDYEING', 'BPCL', 'BRFL', 'BURGERKING', 'BURNPUR', 'BYKE', 'CADILAHC', 'CANBK', 'CASTEXTECH', 'CASTROLIND', 'CCCL', 'CCHHL', 'CDSL', 'CENTEXT', 'CENTRALBK', 'CENTRUM', 'CEREBRAINT', 'CGPOWER', 'CHAMBLFERT', 'CHENNPETRO', 'CHOLAFIN', 'CIPLA', 'COALINDIA', 'COMPINFO', 'CONCOR', 'CONFIPET', 'COX&KINGS', 'CPSEETF', 'CROMPTON', 'CUB', 'CUMMINSIND', 'DAAWAT', 'DABUR', 'DBREALTY', 'DCBBANK', 'DCW', 'DEEPAKFERT', 'DELTACORP', 'DEN', 'DHANBANK', 'DHANI', 'DHFL', 'DIGISPICE', 'DISHTV', 'DLF', 'DNAMEDIA', 'DOLAT', 'DPSCLTD', 'DRREDDY', 'DUCON', 'DWARKESH', 'EASTSILK', 'EASUNREYRL', 'EDELWEISS', 'EDUCOMP', 'EICHERMOT', 'EIHOTEL', 'EKC', 'ELECTCAST', 'EMAMILTD', 'ENERGYDEV', 'ENGINERSIN', 'EQUITAS', 'EQUITASBNK', 'EROSMEDIA', 'ESCORTS', 'EUROCERA', 'EXIDEIND', 'FACT', 'FCONSUMER', 'FCSSOFT', 'FEDERALBNK', 'FEL', 'FILATEX', 'FLFL', 'FORTIS', 'FRETAIL', 'FSL', 'G5', 'GAIL', 'GAMMNINFRA', 'GANESHHOUC', 'GATI', 'GAYAHWS', 'GENUSPAPER', 'GENUSPOWER', 'GEOJITFSL', 'GFLLIMITED', 'GICRE', 'GLENMARK', 'GMDCLTD', 'GMRINFRA', 'GNFC', 'GODREJCP', 'GOENKA', 'GOKUL', 'GOKULAGRO', 'GOLDBEES', 'GPIL', 'GRANULES', 'GRAPHITE', 'GRASIM', 'GREAVESCOT', 'GREENPANEL', 'GREENPLY', 'GREENPOWER', 'GSFC', 'GSPL', 'GTL', 'GTLINFRA', 'GUJGASLTD', 'GVKPIL', 'HAPPSTMNDS', 'HATHWAY', 'HAVELLS', 'HAVISHA', 'HBLPOWER', 'HCC', 'HCL-INSYS', 'HCLTECH', 'HDFC', 'HDFCBANK', 'HDFCLIFE', 'HDFCMFGETF', 'HDIL', 'HEMIPROP', 'HEROMOTOCO', 'HEXATRADEX', 'HFCL', 'HINDALCO', 'HINDCOPPER', 'HINDOILEXP', 'HINDPETRO', 'HINDUNILVR', 'HINDZINC', 'HLVLTD', 'HSCL', 'HTMEDIA', 'HUDCO', 'IBREALEST', 'IBULHSGFIN', 'ICICIBANK', 'ICICIGOLD', 'ICICIPRULI', 'IDBI', 'IDEA', 'IDFC', 'IDFCFIRSTB', 'IEX', 'IFCI', 'IGL', 'IIFL', 'IIFLSEC', 'IL&FSENGG', 'IMAGICAA', 'IMPEXFERRO', 'INDBANK', 'INDHOTEL', 'INDIACEM', 'INDIANB', 'INDIGO', 'INDORAMA', 'INDOSOLAR', 'INDSWFTLTD', 'INDUSINDBK', 'INDUSTOWER', 'INFIBEAM', 'INFY', 'INOXLEISUR', 'INTEGRA', 'INTELLECT', 'INVENTURE', 'IOB', 'IOC', 'IRB', 'IRCON', 'IRCTC', 'IRFC', 'ISMTLTD', 'ITC', 'ITDCEM', 'ITI', 'IVC', 'J&KBANK', 'JAICORPLTD', 'JAINSTUDIO', 'JAMNAAUTO', 'JAYNECOIND', 'JBFIND', 'JINDALSAW', 'JINDALSTEL', 'JISLJALEQS', 'JKPAPER', 'JKTYRE', 'JMFINANCIL', 'JMTAUTOLTD', 'JPASSOCIAT', 'JPINFRATEC', 'JPPOWER', 'JSL', 'JSLHISAR', 'JSWENERGY', 'JSWISPL', 'JSWSTEEL', 'JUMPNET', 'JUSTDIAL', 'JYOTHYLAB', 'JYOTISTRUC', 'KARURVYSYA', 'KCPSUGIND', 'KECL', 'KEERTI', 'KELLTONTEC', 'KESORAMIND', 'KGL', 'KHAICHEM', 'KMSUGAR', 'KOTAKBANK', 'KOTAKBKETF', 'KOTARISUG', 'KPITTECH', 'KRBL', 'KRIDHANINF', 'KSERASERA', 'KTKBANK', 'L&TFH', 'LAURUSLABS', 'LEMONTREE', 'LGBFORGE', 'LICHSGFIN', 'LIQUIDETF', 'LSIL', 'LT', 'LUPIN', 'M&M', 'M&MFIN', 'MAESGETF', 'MAGMA', 'MAHABANK', 'MAN50ETF', 'MANAKCOAT', 'MANALIPETC', 'MANAPPURAM', 'MANGCHEFER', 'MANINFRA', 'MARICO', 'MARINE', 'MARKSANS', 'MAXHEALTH', 'MAZDOCK', 'MBECL', 'MBLINFRA', 'MCDOWELL-N', 'MEGASOFT', 'MEGH', 'MEP', 'MERCATOR', 'MFSL', 'MIC', 'MIRCELECTR', 'MITTAL', 'MMTC', 'MOREPENLAB', 'MOTHERSUMI', 'MRPL', 'MSTCLTD', 'MTEDUCARE', 'MTNL', 'MUTHOOTFIN', 'NACLIND', 'NAGAFERT', 'NATIONALUM', 'NAVKARCORP', 'NBCC', 'NBVENTURES', 'NCC', 'NCPSESDL24', 'NECLIFE', 'NETFIT', 'NETWORK18', 'NFL', 'NHPC', 'NIACL', 'NIFTYBEES', 'NILAINFRA', 'NILASPACES', 'NITINFIRE', 'NLCINDIA', 'NMDC', 'NOCIL', 'NOIDATOLL', 'NTPC', 'OIL', 'ONGC', 'ONMOBILE', 'OPTOCIRCUI', 'ORIENTCEM', 'ORIENTELEC', 'ORIENTPPR', 'ORTEL', 'OSWALAGRO', 'PARACABLES', 'PARSVNATH', 'PATELENG', 'PCJEWELLER', 'PEL', 'PENIND', 'PENINLAND', 'PETRONET', 'PFC', 'PFOCUS', 'PFS', 'PGIL', 'PHILIPCARB', 'PILITA', 'PITTIENG', 'PNB', 'PNBGILTS', 'POWERGRID', 'PRABHAT', 'PRAJIND', 'PRAKASH', 'PRAKASHSTL', 'PRICOLLTD', 'PROZONINTU', 'PRSMJOHNSN', 'PSB', 'PSUBNKBEES', 'PTC', 'PUNJLLOYD', 'PVR', 'RADIOCITY', 'RAIN', 'RAJRAYON', 'RANASUG', 'RAYMOND', 'RBLBANK', 'RCF', 'RCOM', 'RECLTD', 'RELCAPITAL', 'RELIANCE', 'RELIGARE', 'RELINFRA', 'RENUKA', 'RGL', 'RHFL', 'RICOAUTO', 'RMCL', 'RNAVAL', 'ROHITFERRO', 'ROLLT', 'ROLTA', 'RPOWER', 'RTNINFRA', 'RTNPOWER', 'RUCHINFRA', 'RVNL', 'SABEVENTS', 'SADBHAV', 'SADBHIN', 'SAIL', 'SAKUMA', 'SALSTEEL', 'SANCO', 'SANGHIIND', 'SANGINITA', 'SANWARIA', 'SARLAPOLY', 'SATHAISPAT', 'SBICARD', 'SBILIFE', 'SBIN', 'SCAPDVR', 'SCHNEIDER', 'SCI', 'SDBL', 'SEQUENT', 'SETCO', 'SETUINFRA', 'SEZAL', 'SHIL', 'SHIVAMAUTO', 'SHREDIGCEM', 'SHRENIK', 'SHRIRAMEPC', 'SICAL', 'SIL', 'SIMPLEXINF', 'SINTEX', 'SITINET', 'SJVN', 'SKIL', 'SMPL', 'SNOWMAN', 'SOUTHBANK', 'SPCENET', 'SPIC', 'SPICEJET', 'SPMLINFRA', 'SPTL', 'SPYL', 'SREINFRA', 'SRTRANSFIN', 'STAMPEDE', 'STEELXIND', 'SUBEXLTD', 'SUMEETINDS', 'SUNDARAM', 'SUNPHARMA', 'SUNTV', 'SUPREMEENG', 'SUZLON', 'SWSOLAR', 'SYNCOM', 'TAKE', 'TARC', 'TATACHEM', 'TATACOFFEE', 'TATACOMM', 'TATACONSUM', 'TATAMOTORS', 'TATAMTRDVR', 'TATAPOWER', 'TATASTEEL', 'TATASTLBSL', 'TCS', 'TECHM', 'TECHNOFAB', 'TEXMOPIPES', 'TEXRAIL', 'TFCILTD', 'THOMASCOOK', 'TI', 'TIRUMALCHM', 'TIRUPATIFL', 'TITAN', 'TNPETRO', 'TORNTPOWER', 'TRIDENT', 'TRIL', 'TTML', 'TV18BRDCST', 'TVSMOTOR', 'UCOBANK', 'UGARSUGAR', 'UJAAS', 'UJJIVANSFB', 'UNIONBANK', 'UNIPLY', 'UNITECH', 'UPL', 'URJA', 'USHAMART', 'VAKRANGEE', 'VASCONEQ', 'VEDL', 'VIDEOIND', 'VIJIFIN', 'VIKASECO', 'VIKASMCORP', 'VIKASPROP', 'VIKASWSP', 'VIPCLOTHNG', 'VISHAL', 'VIVIDHA', 'VIVIMEDLAB', 'VOLTAS', 'WABAG', 'WEBELSOLAR', 'WELENT', 'WELSPUNIND', 'WINDMACHIN', 'WIPRO', 'YESBANK', 'ZEEL', 'ZEELEARN', 'ZEEMEDIA', 'ZICOM']\n"
     ]
    }
   ],
   "source": [
    "predictor = LiquidityPredictor(\n",
    "    base_dir=\"../downloaded_files\",\n",
    "    lookback=60,\n",
    "    run_date=\"2021-04-01\",\n",
    "    tickers  = []\n",
    ")\n",
    "# Basic filter: only tickers with avg volume >= 1000\n",
    "basic_tickers = predictor.filter_basic(volume_threshold=1000)\n",
    "print(\"Tickers passing basic volume filter:\", basic_tickers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd23d2a-b9a8-4fcc-b294-ee06ba8cb9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked liquid tickers: ['IDEA', 'YESBANK', 'PNB', 'GTLINFRA', 'SAIL', 'TATAMOTORS', 'BHEL', 'SANWARIA', 'IDFCFIRSTB', 'BANKBARODA', 'BALLARPUR', 'KSERASERA', 'TATAPOWER', 'SBIN', 'ITC', 'IRFC', 'IOC', 'IDBI', 'ICICIBANK', 'SUZLON', 'NATIONALUM', 'MERCATOR', 'SOUTHBANK', 'JUMPNET', 'TTML', 'JPPOWER', 'ONGC', 'CANBK', 'ASHOKLEY', 'SITINET', 'NTPC', 'STAMPEDE', 'FEDERALBNK', 'GVKPIL', 'RPOWER', 'GAIL', 'IBULHSGFIN', 'GAYAHWS', 'UCOBANK', 'VEDL', 'TATASTEEL', 'CENTRALBK', 'GAMMNINFRA', 'TRIDENT', 'SUNDARAM', 'FCSSOFT', 'AXISBANK', 'NBCC', 'DLF', 'RCOM', 'VAKRANGEE', 'RBLBANK', 'GMRINFRA', 'L&TFH', 'VIKASMCORP', 'HINDALCO', 'UNIONBANK', 'HFCL', 'COALINDIA', 'SUBEXLTD', 'EASTSILK', 'HDFCBANK', 'BEL', 'ZEEL', 'KGL', 'LSIL', 'MOTHERSUMI', 'RELIANCE', 'IBREALEST', 'NMDC', 'ADANIPOWER', 'CASTEXTECH', 'SPCENET', 'BHARTIARTL', 'RVNL', 'TV18BRDCST', 'IDFC', 'SMPL', 'ADANIPORTS', 'UJAAS', 'GENUSPAPER', 'RTNPOWER', 'WIPRO', 'INDUSINDBK', 'IOB', '3IINFOTECH', 'A2ZINFRA', 'SUNPHARMA', 'ZICOM', 'JINDALSTEL', 'SINTEX', 'ALOKINDS', 'IFCI', 'HDIL', 'ADANIENT', 'JPASSOCIAT', 'MITTAL', 'NCC', 'BANDHANBNK', 'M&MFIN', 'APOLLOTYRE', 'VIVIDHA', 'INFIBEAM', 'GREENPOWER', 'JSWISPL', 'SATHAISPAT', 'CIPLA', 'SUMEETINDS', 'TATAMTRDVR', 'KEERTI', 'INDSWFTLTD', 'UPL', 'TATACHEM', 'INFY', 'BANKINDIA', 'POWERGRID', 'CENTRUM', 'VIKASPROP', 'KELLTONTEC', 'EDUCOMP', 'RANASUG', 'SHRENIK', 'BPCL', 'PFC', 'JSWSTEEL', 'MAHABANK', 'BHANDARI', 'ZEELEARN', 'ISMTLTD', 'UJJIVANSFB', 'NHPC', 'MANAPPURAM', 'ORTEL', 'GOLDBEES', 'BAJAJHIND', 'UNITECH', 'FCONSUMER', 'HINDPETRO', 'SAKUMA', 'MMTC', 'KMSUGAR', 'VIJIFIN', 'AMBUJACEM', 'HSCL', 'LICHSGFIN', 'IIFL', 'NILAINFRA', 'INDBANK', 'HCLTECH', 'ANKITMETAL', 'TATASTLBSL', 'INDIANB', 'DISHTV', 'MIRCELECTR', 'SANGHIIND', 'SETUINFRA', 'CCHHL', 'CGPOWER', 'MARINE', 'INDOSOLAR', 'PARACABLES', 'PUNJLLOYD', 'USHAMART', 'CENTEXT', 'ABCAPITAL', 'EXIDEIND', 'M&M', 'MTEDUCARE', 'TECHM', 'CCCL', 'SUNTV', 'TECHNOFAB', 'ENGINERSIN', 'BOMDYEING', 'SJVN', 'NITINFIRE', 'PRAKASHSTL', 'IMPEXFERRO', 'JSWENERGY', 'JMTAUTOLTD', 'RECLTD', 'FEL', 'KHAICHEM', 'FSL', 'BAGFILMS', 'AKSHOPTFBR', 'ARCHIES', 'SREINFRA', 'ZEEMEDIA', 'KRBL', 'HDFC', 'BCG', 'RCF', 'STEELXIND', 'NACLIND', 'FORTIS', 'MTNL', 'MOREPENLAB', 'AGROPHOS', 'TRIL', 'NECLIFE', 'URJA', 'BILENERGY', 'IVC', 'HINDCOPPER', 'RGL', 'PILITA', 'HATHWAY', 'SICAL', 'UNIPLY', 'INDIACEM', 'BIOCON', 'SEQUENT', 'SHRIRAMEPC', 'ROLTA', 'AVTNPL', 'VIDEOIND', 'RELINFRA', 'SNOWMAN', 'ALPSINDUS', 'KECL', 'RAJRAYON', 'KOTAKBANK', 'PATELENG', 'LT', 'LAURUSLABS', 'NOIDATOLL', 'OPTOCIRCUI', 'VIKASECO', 'CHOLAFIN', 'SIMPLEXINF', 'IRCTC', 'HINDZINC', 'RENUKA', 'KOTARISUG', 'SALSTEEL', 'BYKE', 'SCI', 'TCS', 'GEOJITFSL', 'J&KBANK', 'JKTYRE', 'SPICEJET', 'SHIVAMAUTO', 'PENIND', 'JYOTISTRUC', 'BCP', 'HUDCO', 'GOENKA', 'ROHITFERRO', 'SETCO', 'TIRUPATIFL', 'IRCON', 'RTNINFRA', 'DPSCLTD', 'RAIN', 'INVENTURE', 'MRPL', 'PFOCUS', 'NILASPACES', 'EASUNREYRL', 'TARC', 'DHANBANK', 'FRETAIL', 'HDFCLIFE', 'SANGINITA', 'NCPSESDL24', 'SPTL', 'RHFL', 'MEGH', 'RNAVAL', 'JBFIND', 'PCJEWELLER', 'SRTRANSFIN', 'LGBFORGE', 'RMCL', 'NFL', 'TATACONSUM', 'SUPREMEENG', 'PROZONINTU', 'INDUSTOWER', 'VASCONEQ', 'TEXMOPIPES', 'TVSMOTOR', 'CPSEETF', 'PFS', 'KARURVYSYA', 'CONCOR', 'NETWORK18', 'OSWALAGRO', 'AIRAN', 'SANCO', 'MEGASOFT', 'DELTACORP', 'PTC', 'RUCHINFRA', 'HCC', 'SHIL', 'JISLJALEQS', 'ANDHRACEMT', 'HLVLTD', 'DWARKESH', 'COX&KINGS', 'PRICOLLTD', 'GRANULES', 'COMPINFO', 'LEMONTREE', 'PETRONET', 'MANAKCOAT', 'SARLAPOLY', 'GTL', 'PRAJIND', 'EROSMEDIA', 'KRIDHANINF', 'EUROCERA', 'SABEVENTS', 'JPINFRATEC', 'ASIANPAINT', 'PITTIENG', 'NIFTYBEES', 'SEZAL', 'EDELWEISS', 'GNFC', 'BAJFINANCE', 'CADILAHC', 'NOCIL', 'TATACOFFEE', 'GREAVESCOT', 'PRAKASH', 'SYNCOM', 'ICICIPRULI', 'JAYNECOIND', 'RADIOCITY', 'AUROPHARMA', 'ELECTCAST', 'BRFL', 'SKIL', 'PRABHAT', 'HINDUNILVR', 'HTMEDIA', 'BHARATFORG', 'PHILIPCARB', 'IL&FSENGG', 'ABFRL', 'GMDCLTD', 'JAICORPLTD', 'ORIENTPPR', 'GOKULAGRO', 'ANSALAPI', 'KTKBANK', 'ASHIMASYN', 'BKMINDST', 'UGARSUGAR', 'VIKASWSP', 'CHENNPETRO', 'MBLINFRA', 'HCL-INSYS', 'BLUECHIP', 'LUPIN', 'GANESHHOUC', 'PGIL', 'MANINFRA', 'IGL', 'JINDALSAW', 'GSFC', 'BURNPUR', 'DCW', 'JAINSTUDIO', 'MCDOWELL-N', 'EQUITASBNK', 'HAVELLS', 'HAPPSTMNDS', 'MAESGETF', 'EQUITAS', 'DHFL', 'IEX', 'CDSL', 'GFLLIMITED', 'BURGERKING', 'TORNTPOWER', 'LIQUIDETF', 'VISHAL', 'SADBHIN', 'MIC', 'BODALCHEM', 'WINDMACHIN', 'KESORAMIND', 'IMAGICAA', 'INTEGRA', 'IIFLSEC', 'TAKE', 'DOLAT', 'BALRAMCHIN', 'TITAN', 'MANALIPETC', 'PSUBNKBEES', 'GRAPHITE', 'ARVIND', 'WEBELSOLAR', 'NBVENTURES', 'KCPSUGIND', 'JUSTDIAL', 'PSB', 'HEXATRADEX', 'FLFL', 'DNAMEDIA', 'DUCON', 'INDHOTEL', 'ITDCEM', 'SCAPDVR', 'VOLTAS', 'CASTROLIND', 'INDORAMA', 'JSLHISAR', 'HAVISHA', 'CEREBRAINT', 'SCHNEIDER', 'ATGL', 'MBECL', 'GLENMARK', 'MARICO', 'TIRUMALCHM', 'TI', 'TEXRAIL', 'BLKASHYAP', 'JSL', 'SIL', 'MAGMA', 'SDBL', 'RELCAPITAL', 'FILATEX', 'GRASIM', 'IRB', 'G5', 'TFCILTD', 'CUMMINSIND', 'PARSVNATH', 'ESCORTS', 'CUB', 'DAAWAT', 'INOXLEISUR', 'PEL', 'JYOTHYLAB', 'MAZDOCK', 'KOTAKBKETF', 'BALMLAWRIE', 'EIHOTEL', 'HBLPOWER', 'WELENT', 'NLCINDIA', 'SHREDIGCEM', 'WELSPUNIND', 'GOKUL', 'TNPETRO', 'DABUR', 'KPITTECH', 'NIACL', 'ROLLT', 'MARKSANS', 'GENUSPOWER', 'JKPAPER', 'NAVKARCORP', 'SBILIFE', 'ADANITRANS', 'ACE', 'HEMIPROP', 'SPYL', 'PRSMJOHNSN', 'BEPL', 'DBREALTY', 'ORIENTELEC', 'PVR', 'GATI', 'MAN50ETF', 'ENERGYDEV', 'DRREDDY', 'SPMLINFRA', 'GODREJCP', 'ASHOKA', 'DCBBANK', 'MANGCHEFER', 'BANKBEES', 'MFSL', 'NAGAFERT', 'WABAG', 'CONFIPET', 'ALEMBICLTD', 'HEROMOTOCO', 'GSPL', 'APOLLOHOSP', 'PENINLAND', 'ALCHEM', 'DHANI', 'PNBGILTS', 'DEN', 'MSTCLTD', 'MUTHOOTFIN', 'EICHERMOT', 'VIVIMEDLAB', 'HINDOILEXP', 'OIL', 'ITI', 'AUBANK', 'SPIC', 'JAMNAAUTO', 'THOMASCOOK', 'ANANTRAJ', 'INDIGO', 'DIGISPICE', 'EKC', 'MEP', 'GICRE', 'ONMOBILE', 'RELIGARE', 'FACT', 'RAYMOND', 'GUJGASLTD', 'ORIENTCEM', 'CHAMBLFERT', 'TATACOMM', 'INTELLECT', 'RICOAUTO', 'SBICARD', 'SADBHAV', 'DEEPAKFERT', 'JMFINANCIL', 'ADANIGREEN', 'GREENPLY', 'GPIL', 'HDFCMFGETF', 'CROMPTON', 'BAJAJCON', 'EMAMILTD', 'ICICIGOLD', 'GREENPANEL', 'NETFIT', 'SWSOLAR', 'VIPCLOTHNG', 'MAXHEALTH']\n"
     ]
    }
   ],
   "source": [
    "# Advanced filter: rank tickers by liquidity\n",
    "liquid_tickers = predictor.filter_advanced(\n",
    "    tickers=basic_tickers,\n",
    "    custom_weights={\n",
    "        \"average_volume\": 0.4,\n",
    "        \"trading_frequency\": 0.3,\n",
    "        \"amihud_liquidity\": 0.2,\n",
    "        \"volume_consistency\": 0.1\n",
    "    },\n",
    "    min_score=30.0  # minimum liquidity score to include\n",
    ")\n",
    "\n",
    "print(\"Ranked liquid tickers:\", liquid_tickers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9531338c-337b-48b4-ae35-5526ab0cae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PairTradingAnalyzer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_dir: str,\n",
    "        run_date: str,\n",
    "        lookback: int,\n",
    "        trading_start: str = \"09:15:00\",\n",
    "        trading_end: str = \"15:30:00\",\n",
    "        time_interval_minutes: int = 60,\n",
    "        tickers: Optional[List[str]] = None,\n",
    "        min_mean_reversion: float = 0.5  # <-- add as class attribute\n",
    "    ):\n",
    "        self.base_dir = base_dir\n",
    "        self.run_date = datetime.strptime(run_date, \"%Y-%m-%d\")\n",
    "        self.end_day = self.run_date\n",
    "        self.start_day = self.run_date - timedelta(days=lookback)\n",
    "        self.start_time = datetime.strptime(trading_start, \"%H:%M:%S\").time()\n",
    "        self.end_time = datetime.strptime(trading_end, \"%H:%M:%S\").time()\n",
    "        self.time_interval = timedelta(minutes=time_interval_minutes)\n",
    "        self.tickers = tickers\n",
    "        self.min_mean_reversion = min_mean_reversion  # store it here\n",
    "        self._log_initial_config()\n",
    "\n",
    "\n",
    "    def _log_initial_config(self):\n",
    "        print(\"=\" * 50)\n",
    "        print(\"        PAIR TRADING ANALYZER CONFIGURATION\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Base Directory     : {self.base_dir}\")\n",
    "        print(f\"Run Date           : {self.run_date.date()}\")\n",
    "        print(f\"Start Day          : {self.start_day.date()}\")\n",
    "        print(f\"End Day            : {self.end_day.date()}\")\n",
    "        print(f\"Trading Window     : {self.start_time} â†’ {self.end_time}\")\n",
    "        print(f\"Interval           : {self.time_interval}\")\n",
    "        print(f\"Tickers            : {self.tickers if self.tickers else 'Using default universe'}\")\n",
    "        print(\"=\" * 50)\n",
    "                \n",
    "    def filter_basic(self, volatility_threshold: float = 0.005) -> List[str]:\n",
    "        \"\"\"Return tickers with sufficient price movement.\"\"\"\n",
    "        loader = StockDataLoader(\n",
    "            tickers=self.tickers,\n",
    "            start=f\"{self.start_day.strftime('%Y-%m-%d')} {self.start_time}\",\n",
    "            end=f\"{self.end_day.strftime('%Y-%m-%d')} {self.end_time}\",\n",
    "            select_columns=[\"close\"],\n",
    "            base_dir=self.base_dir\n",
    "        )\n",
    "        data_dict = loader.get_data_for_tickers()\n",
    "        self.data_dict = data_dict  # cache data to avoid reloading\n",
    "        filtered = []\n",
    "\n",
    "        for ticker, df in data_dict.items():\n",
    "            returns = df[\"close\"].pct_change().abs()\n",
    "            avg_volatility = returns.mean()\n",
    "            if avg_volatility >= volatility_threshold:\n",
    "                filtered.append(ticker)\n",
    "        self.filtered_tickers = filtered\n",
    "        print(filtered)\n",
    "        return filtered\n",
    "\n",
    "    def compute_features(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Compute features for clustering.\"\"\"\n",
    "        features = {}\n",
    "        for ticker in self.filtered_tickers:\n",
    "            df = self.data_dict[ticker]\n",
    "            prices = df[\"close\"].to_numpy()\n",
    "            returns = np.diff(prices) / prices[:-1]\n",
    "\n",
    "            # Feature vector: mean-reversion, volatility, autocorrelation\n",
    "            rolling_mean = np.convolve(prices, np.ones(5)/5, mode='valid')\n",
    "            deviations = prices[4:] - rolling_mean\n",
    "            mean_rev_score = np.mean(np.abs(deviations))\n",
    "            vol_score = np.mean(np.abs(returns))\n",
    "            autocorr_score = np.corrcoef(returns[:-1], returns[1:])[0, 1]\n",
    "\n",
    "            features[ticker] = np.array([mean_rev_score, vol_score, autocorr_score])\n",
    "        self.features = features\n",
    "        return features\n",
    "\n",
    "    def cluster_stocks(self, n_clusters: int = 5) -> Dict[int, List[str]]:\n",
    "        \"\"\"Cluster stocks based on feature vectors.\"\"\"\n",
    "        tickers = list(self.features.keys())\n",
    "        X = np.array(list(self.features.values()))\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        clusters = {i: [] for i in range(n_clusters)}\n",
    "        for ticker, label in zip(tickers, labels):\n",
    "            clusters[label].append(ticker)\n",
    "        self.clusters = clusters\n",
    "        return clusters\n",
    "        \n",
    "    def _check_pair(self, t1: str, t2: str) -> Optional[List[str]]:\n",
    "        prices1 = self.data_dict[t1][\"close\"].to_numpy()\n",
    "        prices2 = self.data_dict[t2][\"close\"].to_numpy()\n",
    "    \n",
    "        # Beta hedge ratio\n",
    "        X = add_constant(prices2)\n",
    "        model = OLS(prices1, X).fit()\n",
    "        beta = model.params[1]\n",
    "    \n",
    "        # Spread\n",
    "        spread = prices1 - beta * prices2\n",
    "    \n",
    "        # Cointegration test\n",
    "        score, pvalue, _ = coint(prices1, prices2)\n",
    "        if pvalue >= 0.05:\n",
    "            return None\n",
    "    \n",
    "        # Stationarity test\n",
    "        adf_result = adfuller(spread)\n",
    "        if adf_result[1] >= 0.05:\n",
    "            return None\n",
    "    \n",
    "        # Minimum mean-reversion filter\n",
    "        mean_rev_score = np.mean(np.abs(spread - np.mean(spread)))\n",
    "        if mean_rev_score < self.min_mean_reversion:\n",
    "            return None\n",
    "    \n",
    "        return [t1, t2]\n",
    "    \n",
    "    def find_pairs_in_cluster(self, cluster_tickers: List[str], n_jobs: int = -1) -> List[List[str]]:\n",
    "        \"\"\"Generate pairs of tickers that are cointegrated and stationary in parallel.\"\"\"\n",
    "        \n",
    "        # Generate all unique ticker pairs\n",
    "        ticker_pairs = list(combinations(cluster_tickers, 2))\n",
    "        print(\"Cluster:: \", len(ticker_pairs), cluster_tickers)\n",
    "    \n",
    "        # Use joblib to parallelize the _check_pair function\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(self._check_pair)(t1, t2) for t1, t2 in ticker_pairs\n",
    "        )\n",
    "    \n",
    "        # Filter out None results\n",
    "        pairs = [res for res in results if res is not None]\n",
    "        print(pairs)\n",
    "        print(\"==\"*50)\n",
    "        return pairs\n",
    "        \n",
    "        \n",
    "    def analyze(self, volatility_threshold: float = 0.001, n_clusters: int = 5) -> Dict[str, Dict]:\n",
    "        \"\"\"Full pipeline: filter, compute features, cluster, find pairs.\"\"\"\n",
    "        self.filter_basic(volatility_threshold)\n",
    "        self.compute_features()\n",
    "        self.cluster_stocks(n_clusters=n_clusters)\n",
    "        \n",
    "        # clusters = random.sample(self.clusters, 10)\n",
    "\n",
    "        output = {}\n",
    "        for cluster_id, tickers in self.clusters.items():\n",
    "            \n",
    "            pairs = self.find_pairs_in_cluster(tickers, n_jobs=1)  # sequential\n",
    "            output[f\"cluster_{cluster_id}\"] = {\"tickers\": tickers, \"pairs\": pairs}\n",
    "    \n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5bddc-e80d-4eb3-ad26-f07c038e5223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "        PAIR TRADING ANALYZER CONFIGURATION\n",
      "==================================================\n",
      "Base Directory     : ../downloaded_files\n",
      "Run Date           : 2021-04-01\n",
      "Start Day          : 2021-01-31\n",
      "End Day            : 2021-04-01\n",
      "Trading Window     : 09:15:00 â†’ 15:30:00\n",
      "Interval           : 1:00:00\n",
      "Tickers            : ['IDEA', 'YESBANK', 'PNB', 'GTLINFRA', 'SAIL', 'TATAMOTORS', 'BHEL', 'SANWARIA', 'IDFCFIRSTB', 'BANKBARODA', 'BALLARPUR', 'KSERASERA', 'TATAPOWER', 'SBIN', 'ITC', 'IRFC', 'IOC', 'IDBI', 'ICICIBANK', 'SUZLON', 'NATIONALUM', 'MERCATOR', 'SOUTHBANK', 'JUMPNET', 'TTML', 'JPPOWER', 'ONGC', 'CANBK', 'ASHOKLEY', 'SITINET', 'NTPC', 'STAMPEDE', 'FEDERALBNK', 'GVKPIL', 'RPOWER', 'GAIL', 'IBULHSGFIN', 'GAYAHWS', 'UCOBANK', 'VEDL', 'TATASTEEL', 'CENTRALBK', 'GAMMNINFRA', 'TRIDENT', 'SUNDARAM', 'FCSSOFT', 'AXISBANK', 'NBCC', 'DLF', 'RCOM', 'VAKRANGEE', 'RBLBANK', 'GMRINFRA', 'L&TFH', 'VIKASMCORP', 'HINDALCO', 'UNIONBANK', 'HFCL', 'COALINDIA', 'SUBEXLTD', 'EASTSILK', 'HDFCBANK', 'BEL', 'ZEEL', 'KGL', 'LSIL', 'MOTHERSUMI', 'RELIANCE', 'IBREALEST', 'NMDC', 'ADANIPOWER', 'CASTEXTECH', 'SPCENET', 'BHARTIARTL', 'RVNL', 'TV18BRDCST', 'IDFC', 'SMPL', 'ADANIPORTS', 'UJAAS', 'GENUSPAPER', 'RTNPOWER', 'WIPRO', 'INDUSINDBK', 'IOB', '3IINFOTECH', 'A2ZINFRA', 'SUNPHARMA', 'ZICOM', 'JINDALSTEL', 'SINTEX', 'ALOKINDS', 'IFCI', 'HDIL', 'ADANIENT', 'JPASSOCIAT', 'MITTAL', 'NCC', 'BANDHANBNK', 'M&MFIN', 'APOLLOTYRE', 'VIVIDHA', 'INFIBEAM', 'GREENPOWER', 'JSWISPL', 'SATHAISPAT', 'CIPLA', 'SUMEETINDS', 'TATAMTRDVR', 'KEERTI', 'INDSWFTLTD', 'UPL', 'TATACHEM', 'INFY', 'BANKINDIA', 'POWERGRID', 'CENTRUM', 'VIKASPROP', 'KELLTONTEC', 'EDUCOMP', 'RANASUG', 'SHRENIK', 'BPCL', 'PFC', 'JSWSTEEL', 'MAHABANK', 'BHANDARI', 'ZEELEARN', 'ISMTLTD', 'UJJIVANSFB', 'NHPC', 'MANAPPURAM', 'ORTEL', 'GOLDBEES', 'BAJAJHIND', 'UNITECH', 'FCONSUMER', 'HINDPETRO', 'SAKUMA', 'MMTC', 'KMSUGAR', 'VIJIFIN', 'AMBUJACEM', 'HSCL', 'LICHSGFIN', 'IIFL', 'NILAINFRA', 'INDBANK', 'HCLTECH', 'ANKITMETAL', 'TATASTLBSL', 'INDIANB', 'DISHTV', 'MIRCELECTR', 'SANGHIIND', 'SETUINFRA', 'CCHHL', 'CGPOWER', 'MARINE', 'INDOSOLAR', 'PARACABLES', 'PUNJLLOYD', 'USHAMART', 'CENTEXT', 'ABCAPITAL', 'EXIDEIND', 'M&M', 'MTEDUCARE', 'TECHM', 'CCCL', 'SUNTV', 'TECHNOFAB', 'ENGINERSIN', 'BOMDYEING', 'SJVN', 'NITINFIRE', 'PRAKASHSTL', 'IMPEXFERRO', 'JSWENERGY', 'JMTAUTOLTD', 'RECLTD', 'FEL', 'KHAICHEM', 'FSL', 'BAGFILMS', 'AKSHOPTFBR', 'ARCHIES', 'SREINFRA', 'ZEEMEDIA', 'KRBL', 'HDFC', 'BCG', 'RCF', 'STEELXIND', 'NACLIND', 'FORTIS', 'MTNL', 'MOREPENLAB', 'AGROPHOS', 'TRIL', 'NECLIFE', 'URJA', 'BILENERGY', 'IVC', 'HINDCOPPER', 'RGL', 'PILITA', 'HATHWAY', 'SICAL', 'UNIPLY', 'INDIACEM', 'BIOCON', 'SEQUENT', 'SHRIRAMEPC', 'ROLTA', 'AVTNPL', 'VIDEOIND', 'RELINFRA', 'SNOWMAN', 'ALPSINDUS', 'KECL', 'RAJRAYON', 'KOTAKBANK', 'PATELENG', 'LT', 'LAURUSLABS', 'NOIDATOLL', 'OPTOCIRCUI', 'VIKASECO', 'CHOLAFIN', 'SIMPLEXINF', 'IRCTC', 'HINDZINC', 'RENUKA', 'KOTARISUG', 'SALSTEEL', 'BYKE', 'SCI', 'TCS', 'GEOJITFSL', 'J&KBANK', 'JKTYRE', 'SPICEJET', 'SHIVAMAUTO', 'PENIND', 'JYOTISTRUC', 'BCP', 'HUDCO', 'GOENKA', 'ROHITFERRO', 'SETCO', 'TIRUPATIFL', 'IRCON', 'RTNINFRA', 'DPSCLTD', 'RAIN', 'INVENTURE', 'MRPL', 'PFOCUS', 'NILASPACES', 'EASUNREYRL', 'TARC', 'DHANBANK', 'FRETAIL', 'HDFCLIFE', 'SANGINITA', 'NCPSESDL24', 'SPTL', 'RHFL', 'MEGH', 'RNAVAL', 'JBFIND', 'PCJEWELLER', 'SRTRANSFIN', 'LGBFORGE', 'RMCL', 'NFL', 'TATACONSUM', 'SUPREMEENG', 'PROZONINTU', 'INDUSTOWER', 'VASCONEQ', 'TEXMOPIPES', 'TVSMOTOR', 'CPSEETF', 'PFS', 'KARURVYSYA', 'CONCOR', 'NETWORK18', 'OSWALAGRO', 'AIRAN', 'SANCO', 'MEGASOFT', 'DELTACORP', 'PTC', 'RUCHINFRA', 'HCC', 'SHIL', 'JISLJALEQS', 'ANDHRACEMT', 'HLVLTD', 'DWARKESH', 'COX&KINGS', 'PRICOLLTD', 'GRANULES', 'COMPINFO', 'LEMONTREE', 'PETRONET', 'MANAKCOAT', 'SARLAPOLY', 'GTL', 'PRAJIND', 'EROSMEDIA', 'KRIDHANINF', 'EUROCERA', 'SABEVENTS', 'JPINFRATEC', 'ASIANPAINT', 'PITTIENG', 'NIFTYBEES', 'SEZAL', 'EDELWEISS', 'GNFC', 'BAJFINANCE', 'CADILAHC', 'NOCIL', 'TATACOFFEE', 'GREAVESCOT', 'PRAKASH', 'SYNCOM', 'ICICIPRULI', 'JAYNECOIND', 'RADIOCITY', 'AUROPHARMA', 'ELECTCAST', 'BRFL', 'SKIL', 'PRABHAT', 'HINDUNILVR', 'HTMEDIA', 'BHARATFORG', 'PHILIPCARB', 'IL&FSENGG', 'ABFRL', 'GMDCLTD', 'JAICORPLTD', 'ORIENTPPR', 'GOKULAGRO', 'ANSALAPI', 'KTKBANK', 'ASHIMASYN', 'BKMINDST', 'UGARSUGAR', 'VIKASWSP', 'CHENNPETRO', 'MBLINFRA', 'HCL-INSYS', 'BLUECHIP', 'LUPIN', 'GANESHHOUC', 'PGIL', 'MANINFRA', 'IGL', 'JINDALSAW', 'GSFC', 'BURNPUR', 'DCW', 'JAINSTUDIO', 'MCDOWELL-N', 'EQUITASBNK', 'HAVELLS', 'HAPPSTMNDS', 'MAESGETF', 'EQUITAS', 'DHFL', 'IEX', 'CDSL', 'GFLLIMITED', 'BURGERKING', 'TORNTPOWER', 'LIQUIDETF', 'VISHAL', 'SADBHIN', 'MIC', 'BODALCHEM', 'WINDMACHIN', 'KESORAMIND', 'IMAGICAA', 'INTEGRA', 'IIFLSEC', 'TAKE', 'DOLAT', 'BALRAMCHIN', 'TITAN', 'MANALIPETC', 'PSUBNKBEES', 'GRAPHITE', 'ARVIND', 'WEBELSOLAR', 'NBVENTURES', 'KCPSUGIND', 'JUSTDIAL', 'PSB', 'HEXATRADEX', 'FLFL', 'DNAMEDIA', 'DUCON', 'INDHOTEL', 'ITDCEM', 'SCAPDVR', 'VOLTAS', 'CASTROLIND', 'INDORAMA', 'JSLHISAR', 'HAVISHA', 'CEREBRAINT', 'SCHNEIDER', 'ATGL', 'MBECL', 'GLENMARK', 'MARICO', 'TIRUMALCHM', 'TI', 'TEXRAIL', 'BLKASHYAP', 'JSL', 'SIL', 'MAGMA', 'SDBL', 'RELCAPITAL', 'FILATEX', 'GRASIM', 'IRB', 'G5', 'TFCILTD', 'CUMMINSIND', 'PARSVNATH', 'ESCORTS', 'CUB', 'DAAWAT', 'INOXLEISUR', 'PEL', 'JYOTHYLAB', 'MAZDOCK', 'KOTAKBKETF', 'BALMLAWRIE', 'EIHOTEL', 'HBLPOWER', 'WELENT', 'NLCINDIA', 'SHREDIGCEM', 'WELSPUNIND', 'GOKUL', 'TNPETRO', 'DABUR', 'KPITTECH', 'NIACL', 'ROLLT', 'MARKSANS', 'GENUSPOWER', 'JKPAPER', 'NAVKARCORP', 'SBILIFE', 'ADANITRANS', 'ACE', 'HEMIPROP', 'SPYL', 'PRSMJOHNSN', 'BEPL', 'DBREALTY', 'ORIENTELEC', 'PVR', 'GATI', 'MAN50ETF', 'ENERGYDEV', 'DRREDDY', 'SPMLINFRA', 'GODREJCP', 'ASHOKA', 'DCBBANK', 'MANGCHEFER', 'BANKBEES', 'MFSL', 'NAGAFERT', 'WABAG', 'CONFIPET', 'ALEMBICLTD', 'HEROMOTOCO', 'GSPL', 'APOLLOHOSP', 'PENINLAND', 'ALCHEM', 'DHANI', 'PNBGILTS', 'DEN', 'MSTCLTD', 'MUTHOOTFIN', 'EICHERMOT', 'VIVIMEDLAB', 'HINDOILEXP', 'OIL', 'ITI', 'AUBANK', 'SPIC', 'JAMNAAUTO', 'THOMASCOOK', 'ANANTRAJ', 'INDIGO', 'DIGISPICE', 'EKC', 'MEP', 'GICRE', 'ONMOBILE', 'RELIGARE', 'FACT', 'RAYMOND', 'GUJGASLTD', 'ORIENTCEM', 'CHAMBLFERT', 'TATACOMM', 'INTELLECT', 'RICOAUTO', 'SBICARD', 'SADBHAV', 'DEEPAKFERT', 'JMFINANCIL', 'ADANIGREEN', 'GREENPLY', 'GPIL', 'HDFCMFGETF', 'CROMPTON', 'BAJAJCON', 'EMAMILTD', 'ICICIGOLD', 'GREENPANEL', 'NETFIT', 'SWSOLAR', 'VIPCLOTHNG', 'MAXHEALTH']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "analyzer = PairTradingAnalyzer(\n",
    "    base_dir=\"../downloaded_files\",\n",
    "    run_date=\"2021-04-01\",\n",
    "    lookback=60,\n",
    "    tickers = liquid_tickers\n",
    ")\n",
    "\n",
    "\n",
    "result = analyzer.analyze(volatility_threshold=0.001, n_clusters=5)\n",
    "\n",
    "for cluster_name, cluster_info in result.items():\n",
    "    print(f\"{cluster_name}:\")\n",
    "    print(\"Tickers:\", cluster_info[\"tickers\"])\n",
    "    print(\"Pairs:\", cluster_info[\"pairs\"])\n",
    "    print(\"-\" * 40)\n",
    "# Dump the full result to JSON\n",
    "with open(\"pair_trading_result.json\", \"w\") as f:\n",
    "    json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4279aa-081a-4465-ab2c-44a56a148916",
   "metadata": {},
   "source": [
    "Mean trading volume for all the stocks for the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed0744-0138-4e36-b5b2-2c6a59930b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29c919-731e-457b-84a9-3cf885c68fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
